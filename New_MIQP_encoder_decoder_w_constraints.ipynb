{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.2 kiwisolver-1.4.7 matplotlib-3.9.3 pyparsing-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gurobipy\n",
      "  Downloading gurobipy-12.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
      "Downloading gurobipy-12.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gurobipy\n",
      "Successfully installed gurobipy-12.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.11.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (4.66.5)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch_geometric)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2024.8.30)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.9 aiosignal-1.3.1 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.1 torch_geometric-2.6.1 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install gurobipy\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader  # Corrected import\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_mean_pool  # For pooling in the decoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt  # For plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_QP import my_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n",
      "Set parameter MIPGap to value 0.05\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (linux64 - \"Ubuntu 22.04.5 LTS\")\n",
      "\n",
      "CPU model: AMD EPYC 74F3 24-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 48 physical cores, 96 logical processors, using up to 32 threads\n",
      "\n",
      "Non-default parameters:\n",
      "MIPGap  0.05\n",
      "\n",
      "Optimize a model with 32 rows, 60 columns and 120 nonzeros\n",
      "Model fingerprint: 0x00d24133\n",
      "Model has 464 quadratic objective terms\n",
      "Variable types: 30 continuous, 30 integer (30 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [3e+01, 2e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 5e+00]\n",
      "Found heuristic solution: objective 3654.4800000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 901 rows, 526 columns, 2321 nonzeros\n",
      "Presolved model has 30 quadratic constraint(s)\n",
      "Presolved model has 434 bilinear constraint(s)\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 496 continuous, 30 integer (30 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 1 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                      31.9607000    0.00000   100%     -    0s\n",
      "*    0     0               0      31.9607000    0.00000   100%     -    0s\n",
      "H    0     0                      20.8607250    0.00000   100%     -    0s\n",
      "*    0     0               0      20.8607250    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    2   20.86072    0.00000   100%     -    0s\n",
      "H    0     0                      18.2729563    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "H    0     0                      17.6415556    0.00000   100%     -    0s\n",
      "H    0     0                      17.4674945    0.00000   100%     -    0s\n",
      "H    0     0                      17.4402731    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   10   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.08623    0  149   17.44027    0.08623   100%     -    0s\n",
      "     0     0    1.81173    0   10   17.44027    1.81173  89.6%     -    0s\n",
      "     0     0    1.81174    0   17   17.44027    1.81174  89.6%     -    0s\n",
      "     0     0    2.62598    0   33   17.44027    2.62598  84.9%     -    0s\n",
      "     0     0    3.56723    0   25   17.44027    3.56723  79.5%     -    0s\n",
      "     0     0    3.93799    0   10   17.44027    3.93799  77.4%     -    0s\n",
      "     0     0    4.38555    0   22   17.44027    4.38555  74.9%     -    0s\n",
      "     0     0    5.55565    0   22   17.44027    5.55565  68.1%     -    0s\n",
      "     0     0    7.15470    0   27   17.44027    7.15470  59.0%     -    0s\n",
      "     0     0    7.69619    0   20   17.44027    7.69619  55.9%     -    0s\n",
      "     0     0    7.73872    0   29   17.44027    7.73872  55.6%     -    0s\n",
      "     0     0    7.78685    0   29   17.44027    7.78685  55.4%     -    0s\n",
      "     0     0    7.79280    0   29   17.44027    7.79280  55.3%     -    0s\n",
      "     0     0    7.80101    0   29   17.44027    7.80101  55.3%     -    0s\n",
      "H    0     0                      16.3118000    7.80101  52.2%     -    0s\n",
      "     0     0    7.80101    0   29   16.31180    7.80101  52.2%     -    0s\n",
      "H    0     0                      16.2628544    7.80101  52.0%     -    0s\n",
      "H    0     0                      15.9880975    7.80101  51.2%     -    0s\n",
      "H    0     0                      15.7081129    7.80101  50.3%     -    0s\n",
      "     0     2    7.80101    0   29   15.70811    7.80101  50.3%     -    0s\n",
      "H 2033  1602                      15.6653635    8.60559  45.1%  12.3    0s\n",
      "H 2190  1602                      15.6644219    8.71024  44.4%  12.4    0s\n",
      "H 2280  1533                      15.5343914    8.74770  43.7%  12.4    0s\n",
      "H 2284  1442                      15.4851673    8.74770  43.5%  12.4    0s\n",
      "H 2286  1370                      15.4734961   11.00119  28.9%  12.4    0s\n",
      "H 2287  1303                      15.4657935   12.37306  20.0%  12.4    0s\n",
      "H 2301  1246                      15.4645115   14.66410  5.18%  12.3    2s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 1\n",
      "  Flow cover: 4\n",
      "  RLT: 18\n",
      "  BQP: 1\n",
      "  PSD: 145\n",
      "\n",
      "Explored 2303 nodes (30934 simplex iterations) in 2.65 seconds (2.31 work units)\n",
      "Thread count was 32 (of 96 available processors)\n",
      "\n",
      "Solution count 10: 15.4645 15.4658 15.4735 ... 16.2629\n",
      "\n",
      "Optimal solution found (tolerance 5.00e-02)\n",
      "Best objective 1.546451154715e+01, best bound 1.469170048548e+01, gap 4.9973%\n",
      "\n",
      "User-callback calls 5908, time in user-callback 0.07 sec\n"
     ]
    }
   ],
   "source": [
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "# Solution storage\n",
    "grb_model._feasible_solutions = []\n",
    "grb_model._relaxation_solutions = []\n",
    "grb_model.setParam(\"MIPGap\", 0.05)\n",
    "#model.setParam(\"NodeLimit\", 100)  # Explore a limited number of nodes\n",
    "\n",
    "# Optimize\n",
    "grb_model.optimize(my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.0, 0.0, 0.49055482068657474, 0.0, 0.0, 0.09207045008492966, 0.0, 0.0, 0.06558348815658188, 0.2063472207852159, 0.0, 0.14544402028669787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 1.0, -0.0, -0.0, 1.0, 0.0, 0.0, 1.0, 1.0, -0.0, 1.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve optimal solution if available\n",
    "if grb_model.status == GRB.OPTIMAL:\n",
    "    optimal_solution = grb_model.getAttr('X', grb_model.getVars())\n",
    "    print(\"Optimal solution:\", optimal_solution)\n",
    "else:\n",
    "    print(f\"Model status: {grb_model.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n"
     ]
    }
   ],
   "source": [
    "## Extracting bounds\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "variable_bounds = {}\n",
    "for var in grb_model.getVars():\n",
    "    variable_bounds[var.VarName] = {'Lower': var.LB, 'Upper': var.UB}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Q, A, b, d, etc\n",
    "\n",
    "from parse_QP import parse_qplib_file\n",
    "\n",
    "# Replace '0031' with the desired file number\n",
    "data = parse_qplib_file('0031')\n",
    "\n",
    "# Access the data\n",
    "A = data['A']\n",
    "b_vector = data['b_vector']\n",
    "E = data['E']\n",
    "d_vector = data['d']\n",
    "Q = data['Q']\n",
    "variables_info = data['variables_info']\n",
    "variables_info = [v[0] for v in variables_info]\n",
    "binary_indices = data['binary_indices']\n",
    "variable_indices = data['variable_indices']\n",
    "\n",
    "m, n = A.shape\n",
    "# Get indices of non-zero elements in A\n",
    "row_indices, col_indices = np.nonzero(A)\n",
    "edge_weights = A[row_indices, col_indices]\n",
    "\n",
    "# Map variable types to numerical values\n",
    "# Node types: 0 - continuous, 1 - binary\n",
    "variable_types = np.array([0 if v[0] == 'x' else 1 for v in variables_info])\n",
    "\n",
    "# Collect indices of continuous and binary variables\n",
    "continuous_indices = np.where(variable_types == 0)[0]\n",
    "binary_indices = np.where(variable_types == 1)[0]\n",
    "n_continuous = len(continuous_indices)\n",
    "n_binary = len(binary_indices)\n",
    "n_variables = n_continuous + n_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing feasible solutions from file.\n",
      "Loaded existing infeasible solutions from file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from generate_solutions import generate_feasible_solutions, generate_infeasible_solutions\n",
    "\n",
    "# Set generate_new to False by default\n",
    "generate_new = False\n",
    "feasible_data_file = 'feasible_data.pkl'\n",
    "infeasible_data_file = 'infeasible_data.pkl'\n",
    "num_samples = 5000\n",
    "\n",
    "# Load or generate feasible solutions (as in your original code)\n",
    "if not generate_new and os.path.exists(feasible_data_file):\n",
    "    # Load feasible data\n",
    "    with open(feasible_data_file, 'rb') as f:\n",
    "        feasible_data = pickle.load(f)\n",
    "    feasible_solutions = feasible_data['solutions']\n",
    "    feasible_costs = feasible_data['costs']\n",
    "    print(\"Loaded existing feasible solutions from file.\")\n",
    "else:\n",
    "    # Generate feasible solutions\n",
    "    num_objectives = num_samples  # Adjust the number as needed\n",
    "    feasible_solutions, feasible_costs = generate_feasible_solutions(\n",
    "        A, E, Q, variables_info, b_vector, d_vector, num_objectives\n",
    "    )\n",
    "    # Save the generated data for future use\n",
    "    feasible_data = {'solutions': feasible_solutions, 'costs': feasible_costs}\n",
    "    with open(feasible_data_file, 'wb') as f:\n",
    "        pickle.dump(feasible_data, f)\n",
    "    print(\"Generated and saved feasible solutions.\")\n",
    "\n",
    "# Now, load or generate infeasible solutions\n",
    "if not generate_new and os.path.exists(infeasible_data_file):\n",
    "    # Load infeasible data\n",
    "    with open(infeasible_data_file, 'rb') as f:\n",
    "        infeasible_data = pickle.load(f)\n",
    "    infeasible_solutions = infeasible_data['solutions']\n",
    "    infeasible_costs = infeasible_data['costs']\n",
    "    print(\"Loaded existing infeasible solutions from file.\")\n",
    "else:\n",
    "    # Generate infeasible solutions\n",
    "    num_infeasible_samples = num_samples  # Adjust as needed\n",
    "    infeasible_solutions, infeasible_costs = generate_infeasible_solutions(\n",
    "        A, E, variables_info, b_vector, d_vector, Q, num_infeasible_samples, feasible_solutions\n",
    "    )\n",
    "    # Save the generated data for future use\n",
    "    infeasible_data = {'solutions': infeasible_solutions, 'costs': infeasible_costs}\n",
    "    with open(infeasible_data_file, 'wb') as f:\n",
    "        pickle.dump(infeasible_data, f)\n",
    "    print(\"Generated and saved infeasible solutions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Test Loss: 3.0220, Recon Loss: 1.0075, Cost Loss: 1.0087, Constraint Loss: 1.0059\n",
      "----\n",
      "Epoch 1, Train Loss: 0.6805, Recon Loss: 0.0852, Cost Loss: 0.3221, Constraint Loss: 0.2732\n",
      "Test Loss: 0.1318, Recon Loss: 0.0002, Cost Loss: 0.1196, Constraint Loss: 0.0120\n",
      "----\n",
      "----\n",
      "Epoch 2, Train Loss: 0.1325, Recon Loss: 0.0001, Cost Loss: 0.1234, Constraint Loss: 0.0089\n",
      "Test Loss: 0.1049, Recon Loss: 0.0003, Cost Loss: 0.0957, Constraint Loss: 0.0089\n",
      "----\n",
      "----\n",
      "Epoch 3, Train Loss: 0.1087, Recon Loss: 0.0001, Cost Loss: 0.1008, Constraint Loss: 0.0077\n",
      "Test Loss: 0.1009, Recon Loss: 0.0000, Cost Loss: 0.0930, Constraint Loss: 0.0079\n",
      "----\n",
      "----\n",
      "Epoch 4, Train Loss: 0.0916, Recon Loss: 0.0001, Cost Loss: 0.0842, Constraint Loss: 0.0074\n",
      "Test Loss: 0.0778, Recon Loss: 0.0000, Cost Loss: 0.0700, Constraint Loss: 0.0078\n",
      "----\n",
      "----\n",
      "Epoch 5, Train Loss: 0.0819, Recon Loss: 0.0001, Cost Loss: 0.0748, Constraint Loss: 0.0069\n",
      "Test Loss: 0.0704, Recon Loss: 0.0000, Cost Loss: 0.0632, Constraint Loss: 0.0072\n",
      "----\n",
      "----\n",
      "Epoch 6, Train Loss: 0.0733, Recon Loss: 0.0002, Cost Loss: 0.0648, Constraint Loss: 0.0083\n",
      "Test Loss: 0.1383, Recon Loss: 0.0001, Cost Loss: 0.1253, Constraint Loss: 0.0128\n",
      "----\n",
      "----\n",
      "Epoch 7, Train Loss: 0.0673, Recon Loss: 0.0002, Cost Loss: 0.0579, Constraint Loss: 0.0091\n",
      "Test Loss: 0.0609, Recon Loss: 0.0007, Cost Loss: 0.0407, Constraint Loss: 0.0194\n",
      "----\n",
      "----\n",
      "Epoch 8, Train Loss: 0.0500, Recon Loss: 0.0003, Cost Loss: 0.0409, Constraint Loss: 0.0088\n",
      "Test Loss: 0.0609, Recon Loss: 0.0002, Cost Loss: 0.0535, Constraint Loss: 0.0071\n",
      "----\n",
      "----\n",
      "Epoch 9, Train Loss: 0.0379, Recon Loss: 0.0002, Cost Loss: 0.0304, Constraint Loss: 0.0073\n",
      "Test Loss: 0.0385, Recon Loss: 0.0007, Cost Loss: 0.0282, Constraint Loss: 0.0096\n",
      "----\n",
      "----\n",
      "Epoch 10, Train Loss: 0.0312, Recon Loss: 0.0004, Cost Loss: 0.0238, Constraint Loss: 0.0070\n",
      "Test Loss: 0.0224, Recon Loss: 0.0004, Cost Loss: 0.0149, Constraint Loss: 0.0071\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import numpy as np\n",
    "\n",
    "# Adjusted GNNModelObj to output variable node embeddings\n",
    "class GNNModelObj(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNNModelObj, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))\n",
    "        return x  # Return embeddings for all variable nodes\n",
    "\n",
    "# Adjusted GNNModelConstraints to output variable and constraint node embeddings\n",
    "class GNNModelConstraints(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNNModelConstraints, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))\n",
    "        x_var = x[data.variable_mask]\n",
    "        x_constraints = x[~data.variable_mask]\n",
    "        return x_var, x_constraints  # Return embeddings for both variables and constraints\n",
    "\n",
    "# Define the JointGNN model\n",
    "class JointGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels_obj, hidden_channels_cons, decoder_hidden_channels):\n",
    "        super(JointGNN, self).__init__()\n",
    "        # Encoders\n",
    "        self.encoder_obj = GNNModelObj(hidden_channels_obj)\n",
    "        self.encoder_cons = GNNModelConstraints(hidden_channels_cons)\n",
    "        # Decoder for x reconstruction\n",
    "        concat_dim = hidden_channels_obj + hidden_channels_cons\n",
    "        self.decoder_x = torch.nn.Sequential(\n",
    "            torch.nn.Linear(concat_dim, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "        # Decoder for cost prediction\n",
    "        self.decoder_cost = torch.nn.Sequential(\n",
    "            torch.nn.Linear(concat_dim, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "        # Decoder for constraint violation prediction\n",
    "        self.decoder_constraints = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels_cons, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_obj, data_feas):\n",
    "        # Process data through encoders\n",
    "        x_obj = self.encoder_obj(data_obj)  # Embeddings for variables\n",
    "        x_cons_var, x_cons_constraints = self.encoder_cons(data_feas)  # Embeddings for variables and constraints\n",
    "        # Extract variable embeddings\n",
    "        x_obj_var = x_obj[data_obj.variable_mask]\n",
    "        # Concatenate per-variable embeddings\n",
    "        x_var = torch.cat([x_obj_var, x_cons_var], dim=1)\n",
    "        # Decode to reconstruct x\n",
    "        x_hat = self.decoder_x(x_var).squeeze()\n",
    "        # Compute mean pooling over variable nodes for cost prediction\n",
    "        batch = data_obj.batch[data_obj.variable_mask]\n",
    "        x_var_pooled = global_mean_pool(x_var, batch)  # [batch_size, concat_dim]\n",
    "        # Predict cost\n",
    "        predicted_cost = self.decoder_cost(x_var_pooled).squeeze()\n",
    "        # Decode to predict constraint violations\n",
    "        predicted_constraints = self.decoder_constraints(x_cons_constraints).squeeze()\n",
    "        return x_hat, predicted_cost, predicted_constraints\n",
    "\n",
    "# Custom dataset to return pairs of data_obj and data_feas\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, data_list_obj, data_list_feas):\n",
    "        assert len(data_list_obj) == len(data_list_feas)\n",
    "        self.data_list_obj = data_list_obj\n",
    "        self.data_list_feas = data_list_feas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_obj)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list_obj[idx], self.data_list_feas[idx]\n",
    "\n",
    "# Custom collate function for batching\n",
    "def joint_collate_fn(batch):\n",
    "    data_obj_list, data_feas_list = zip(*batch)\n",
    "    batch_obj = Batch.from_data_list(data_obj_list)\n",
    "    batch_feas = Batch.from_data_list(data_feas_list)\n",
    "    return batch_obj, batch_feas\n",
    "\n",
    "# Data preparation functions\n",
    "def prepare_edge_index_and_attr(Q):\n",
    "    n_variables = Q.shape[0]\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    Q = Q if isinstance(Q, np.ndarray) else Q.toarray()\n",
    "    for i in range(n_variables):\n",
    "        for j in range(n_variables):\n",
    "            if Q[i, j] != 0:\n",
    "                edge_index.append([i, j])\n",
    "                edge_attr.append(Q[i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def prepare_constraint_edge_data(A, E, n_variables):\n",
    "    # A: m x n, E: p x n\n",
    "    m = A.shape[0]\n",
    "    p = E.shape[0]\n",
    "    num_constraints = m + p\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    # Edges from variables to inequality constraints\n",
    "    for constraint_idx in range(m):\n",
    "        for variable_idx in range(n_variables):\n",
    "            coeff = A[constraint_idx, variable_idx]\n",
    "            if coeff != 0:\n",
    "                # Edge from variable to constraint\n",
    "                edge_index.append([variable_idx, n_variables + constraint_idx])\n",
    "                edge_attr.append(coeff)\n",
    "    # Edges from variables to equality constraints\n",
    "    for constraint_idx in range(p):\n",
    "        for variable_idx in range(n_variables):\n",
    "            coeff = E[constraint_idx, variable_idx]\n",
    "            if coeff != 0:\n",
    "                # Edge from variable to constraint\n",
    "                edge_index.append([variable_idx, n_variables + m + constraint_idx])\n",
    "                edge_attr.append(coeff)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def create_data_list_obj(solutions, costs, edge_index, edge_attr):\n",
    "    data_list = []\n",
    "    for x_sol, cost_sol in zip(solutions, costs):\n",
    "        x = torch.tensor(x_sol, dtype=torch.float).unsqueeze(-1)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.variable_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        data.y_x = x.squeeze()\n",
    "        data.y_cost = torch.tensor([cost_sol], dtype=torch.float)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def create_data_list_feas(solutions, costs, A, E, b_vector, d_vector, edge_index, edge_attr, n_variables):\n",
    "    data_list = []\n",
    "    m = A.shape[0]\n",
    "    p = E.shape[0]\n",
    "    num_constraints = m + p\n",
    "    for x_sol, cost_sol in zip(solutions, costs):\n",
    "        x_vars = torch.tensor(x_sol, dtype=torch.float).unsqueeze(-1)\n",
    "        # Constraint node features: b_vector and d_vector\n",
    "        b = torch.tensor(b_vector, dtype=torch.float).unsqueeze(-1)\n",
    "        d = torch.tensor(d_vector, dtype=torch.float).unsqueeze(-1)\n",
    "        x_constraints = torch.cat([b, d], dim=0)\n",
    "        x_total = torch.cat([x_vars, x_constraints], dim=0)\n",
    "        data = Data(x=x_total, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.variable_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.variable_mask[:n_variables] = True  # Variables are first\n",
    "        data.y_x = x_vars.squeeze()\n",
    "        data.y_cost = torch.tensor([cost_sol], dtype=torch.float)\n",
    "\n",
    "        # Compute constraint violations\n",
    "        A_tensor = torch.tensor(A, dtype=torch.float)\n",
    "        E_tensor = torch.tensor(E, dtype=torch.float)\n",
    "        inequality_violations = torch.mv(A_tensor, x_vars.squeeze()) - torch.tensor(b_vector, dtype=torch.float)\n",
    "        equality_violations = torch.mv(E_tensor, x_vars.squeeze()) - torch.tensor(d_vector, dtype=torch.float)\n",
    "        y_constraints = torch.cat([inequality_violations, equality_violations], dim=0)\n",
    "        data.y_constraints = y_constraints  # [num_constraints]\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def normalize_node_features(data_list):\n",
    "    # Concatenate all node features from all graphs\n",
    "    x_all = torch.cat([data.x for data in data_list], dim=0)\n",
    "    mean = x_all.mean(dim=0)\n",
    "    std = x_all.std(dim=0) + 1e-6  # Avoid division by zero\n",
    "    for data in data_list:\n",
    "        data.x = (data.x - mean) / std\n",
    "    return data_list, mean, std\n",
    "\n",
    "def normalize_targets(data_list):\n",
    "    # Normalize y_x (node targets)\n",
    "    y_x_all = torch.cat([data.y_x for data in data_list], dim=0)\n",
    "    mean_y_x = y_x_all.mean()\n",
    "    std_y_x = y_x_all.std() + 1e-6\n",
    "    for data in data_list:\n",
    "        data.y_x = (data.y_x - mean_y_x) / std_y_x\n",
    "\n",
    "    # Normalize y_cost (graph targets)\n",
    "    y_cost_all = torch.cat([data.y_cost for data in data_list], dim=0)\n",
    "    mean_y_cost = y_cost_all.mean()\n",
    "    std_y_cost = y_cost_all.std() + 1e-6\n",
    "    for data in data_list:\n",
    "        data.y_cost = (data.y_cost - mean_y_cost) / std_y_cost\n",
    "\n",
    "    # Initialize means and stds for y_constraints\n",
    "    mean_y_constraints = None\n",
    "    std_y_constraints = None\n",
    "\n",
    "    # Normalize y_constraints if present\n",
    "    if hasattr(data_list[0], 'y_constraints'):\n",
    "        y_constraints_all = torch.cat([data.y_constraints for data in data_list], dim=0)\n",
    "        mean_y_constraints = y_constraints_all.mean()\n",
    "        std_y_constraints = y_constraints_all.std() + 1e-6\n",
    "        for data in data_list:\n",
    "            data.y_constraints = (data.y_constraints - mean_y_constraints) / std_y_constraints\n",
    "\n",
    "    return data_list, (mean_y_x, std_y_x), (mean_y_cost, std_y_cost), (mean_y_constraints, std_y_constraints)\n",
    "\n",
    "def split_data(data_list, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(data_list))\n",
    "    test_set_size = int(len(data_list) * test_size)\n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "    train_data = [data_list[i] for i in train_indices]\n",
    "    test_data = [data_list[i] for i in test_indices]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Assume you have the following variables defined:\n",
    "# Q, A, E, b_vector, d_vector, feasible_solutions, feasible_costs, infeasible_solutions, infeasible_costs, n_variables\n",
    "\n",
    "# Prepare edge information from Q\n",
    "edge_index_obj, edge_attr_obj = prepare_edge_index_and_attr(Q)\n",
    "\n",
    "# Prepare edge information from A and E\n",
    "edge_index_feas, edge_attr_feas = prepare_constraint_edge_data(A, E, n_variables)\n",
    "\n",
    "# Create Data objects for feasible solutions\n",
    "data_list_obj_feas = create_data_list_obj(feasible_solutions, feasible_costs, edge_index_obj, edge_attr_obj)\n",
    "data_list_feas_feas = create_data_list_feas(feasible_solutions, feasible_costs, A, E, b_vector, d_vector, edge_index_feas, edge_attr_feas, n_variables)\n",
    "\n",
    "# Create Data objects for infeasible solutions\n",
    "data_list_obj_infeas = create_data_list_obj(infeasible_solutions, infeasible_costs, edge_index_obj, edge_attr_obj)\n",
    "data_list_feas_infeas = create_data_list_feas(infeasible_solutions, infeasible_costs, A, E, b_vector, d_vector, edge_index_feas, edge_attr_feas, n_variables)\n",
    "\n",
    "# Combine feasible and infeasible data\n",
    "data_list_obj = data_list_obj_feas + data_list_obj_infeas\n",
    "data_list_feas = data_list_feas_feas + data_list_feas_infeas\n",
    "\n",
    "# Normalize node features for data_list_obj\n",
    "data_list_obj, mean_obj, std_obj = normalize_node_features(data_list_obj)\n",
    "\n",
    "# Normalize node features for data_list_feas\n",
    "data_list_feas, mean_feas, std_feas = normalize_node_features(data_list_feas)\n",
    "\n",
    "# Normalize targets (x and cost) for data_list_obj\n",
    "data_list_obj, (mean_y_x, std_y_x), (mean_y_cost, std_y_cost), _ = normalize_targets(data_list_obj)\n",
    "\n",
    "# Normalize targets (x, cost, and constraints) for data_list_feas\n",
    "data_list_feas, _, _, (mean_y_constraints, std_y_constraints) = normalize_targets(data_list_feas)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data_obj, test_data_obj = split_data(data_list_obj, test_size=0.2, random_state=42)\n",
    "train_data_feas, test_data_feas = split_data(data_list_feas, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = JointDataset(train_data_obj, train_data_feas)\n",
    "test_dataset = JointDataset(test_data_obj, test_data_feas)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=joint_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=joint_collate_fn)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = JointGNN(hidden_channels_obj=256, hidden_channels_cons=128, decoder_hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    total_constraint_loss = 0\n",
    "    total_original_cost_loss = 0\n",
    "    total_original_constraint_loss = 0\n",
    "    for data_obj_batch, data_feas_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "        # Get target x values and cost values\n",
    "        y_x = data_obj_batch.y_x\n",
    "        y_cost = data_obj_batch.y_cost.squeeze()\n",
    "        y_constraints = data_feas_batch.y_constraints\n",
    "        # Compute reconstruction loss\n",
    "        recon_loss = criterion(x_hat, y_x)\n",
    "        # Compute cost prediction loss\n",
    "        cost_loss = criterion(predicted_cost, y_cost)\n",
    "        # Compute constraint violation loss\n",
    "        constraint_loss = criterion(predicted_constraints, y_constraints)\n",
    "        # Total loss (weighted sum)\n",
    "        loss = recon_loss + cost_loss + constraint_loss  # You can adjust weights if desired\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data_obj_batch.num_graphs\n",
    "        total_recon_loss += recon_loss.item() * data_obj_batch.num_graphs\n",
    "        total_cost_loss += cost_loss.item() * data_obj_batch.num_graphs\n",
    "        total_constraint_loss += constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "        # Compute original cost loss (denormalized)\n",
    "        predicted_cost_orig = predicted_cost * std_y_cost + mean_y_cost\n",
    "        y_cost_orig = y_cost * std_y_cost + mean_y_cost\n",
    "        original_cost_loss = criterion(predicted_cost_orig, y_cost_orig)\n",
    "        total_original_cost_loss += original_cost_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "        # Compute original constraint loss (denormalized)\n",
    "        predicted_constraints_orig = predicted_constraints * std_y_constraints + mean_y_constraints\n",
    "        y_constraints_orig = y_constraints * std_y_constraints + mean_y_constraints\n",
    "        original_constraint_loss = criterion(predicted_constraints_orig, y_constraints_orig)\n",
    "        total_original_constraint_loss += original_constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_cost_loss = total_cost_loss / len(train_loader.dataset)\n",
    "    avg_constraint_loss = total_constraint_loss / len(train_loader.dataset)\n",
    "    avg_original_cost_loss = total_original_cost_loss / len(train_loader.dataset)\n",
    "    avg_original_constraint_loss = total_original_constraint_loss / len(train_loader.dataset)\n",
    "    return avg_loss, avg_recon_loss, avg_cost_loss, avg_constraint_loss, avg_original_cost_loss, avg_original_constraint_loss\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    total_constraint_loss = 0\n",
    "    total_original_cost_loss = 0\n",
    "    total_original_constraint_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data_obj_batch, data_feas_batch in loader:\n",
    "            x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "            y_x = data_obj_batch.y_x\n",
    "            y_cost = data_obj_batch.y_cost.squeeze()\n",
    "            y_constraints = data_feas_batch.y_constraints\n",
    "            recon_loss = criterion(x_hat, y_x)\n",
    "            cost_loss = criterion(predicted_cost, y_cost)\n",
    "            constraint_loss = criterion(predicted_constraints, y_constraints)\n",
    "            loss = recon_loss + cost_loss + constraint_loss\n",
    "            total_loss += loss.item() * data_obj_batch.num_graphs\n",
    "            total_recon_loss += recon_loss.item() * data_obj_batch.num_graphs\n",
    "            total_cost_loss += cost_loss.item() * data_obj_batch.num_graphs\n",
    "            total_constraint_loss += constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "            # Compute original cost loss (denormalized)\n",
    "            predicted_cost_orig = predicted_cost * std_y_cost + mean_y_cost\n",
    "            y_cost_orig = y_cost * std_y_cost + mean_y_cost\n",
    "            original_cost_loss = criterion(predicted_cost_orig, y_cost_orig)\n",
    "            total_original_cost_loss += original_cost_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "            # Compute original constraint loss (denormalized)\n",
    "            predicted_constraints_orig = predicted_constraints * std_y_constraints + mean_y_constraints\n",
    "            y_constraints_orig = y_constraints * std_y_constraints + mean_y_constraints\n",
    "            original_constraint_loss = criterion(predicted_constraints_orig, y_constraints_orig)\n",
    "            total_original_constraint_loss += original_constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(loader.dataset)\n",
    "    avg_cost_loss = total_cost_loss / len(loader.dataset)\n",
    "    avg_constraint_loss = total_constraint_loss / len(loader.dataset)\n",
    "    avg_original_cost_loss = total_original_cost_loss / len(loader.dataset)\n",
    "    avg_original_constraint_loss = total_original_constraint_loss / len(loader.dataset)\n",
    "    return avg_loss, avg_recon_loss, avg_cost_loss, avg_constraint_loss, avg_original_cost_loss, avg_original_constraint_loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "test_loss, test_recon_loss, test_cost_loss, test_constraint_loss, test_orig_cost_loss, test_orig_constraint_loss = test(test_loader)\n",
    "\n",
    "print(f\"Pre-Test Loss: {test_loss:.4f}, Recon Loss: {test_recon_loss:.4f}, \"\n",
    "      f\"Cost Loss: {test_cost_loss:.4f}, Constraint Loss: {test_constraint_loss:.4f}\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Unpack all six returned values\n",
    "    train_loss, train_recon_loss, train_cost_loss, train_constraint_loss, train_orig_cost_loss, train_orig_constraint_loss = train()\n",
    "    test_loss, test_recon_loss, test_cost_loss, test_constraint_loss, test_orig_cost_loss, test_orig_constraint_loss = test(test_loader)\n",
    "\n",
    "    print(\"----\")\n",
    "    # Print out all the losses\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Recon Loss: {train_recon_loss:.4f}, \"\n",
    "          f\"Cost Loss: {train_cost_loss:.4f}, Constraint Loss: {train_constraint_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Recon Loss: {test_recon_loss:.4f}, \"\n",
    "          f\"Cost Loss: {test_cost_loss:.4f}, Constraint Loss: {test_constraint_loss:.4f}\")\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Reconstruction Error: 0.17192766070365906\n"
     ]
    }
   ],
   "source": [
    "# Use a batch of data\n",
    "data_obj_batch, data_feas_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    # Encode and decode\n",
    "    z_obj = model.encoder_obj(data_obj_batch)\n",
    "    z_cons_var, z_constraints = model.encoder_cons(data_feas_batch)\n",
    "    x_obj_var = z_obj[data_obj_batch.variable_mask]\n",
    "    x_var = torch.cat([x_obj_var, z_cons_var], dim=1)\n",
    "    x_hat = model.decoder_x(x_var).squeeze()\n",
    "    # Denormalize x_hat and y_x\n",
    "    x_hat_denorm = x_hat * std_y_x + mean_y_x\n",
    "    y_x_denorm = data_obj_batch.y_x * std_y_x + mean_y_x\n",
    "    # Compute reconstruction error\n",
    "    recon_error = torch.norm(x_hat_denorm - y_x_denorm)\n",
    "    print(f\"Decoder Reconstruction Error: {recon_error.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7b6da0916c90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_x_denorm.cpu().numpy(), np.round(x_hat_denorm, 3).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(model, data_list_obj, data_list_feas):\n",
    "    model.eval()\n",
    "    actual_costs = []\n",
    "    predicted_costs = []\n",
    "    actual_constraints = []\n",
    "    predicted_constraints_list = []  # Renamed the list\n",
    "    with torch.no_grad():\n",
    "        for data_obj, data_feas in zip(data_list_obj, data_list_feas):\n",
    "            # Get the true x\n",
    "            y_x = data_obj.y_x * std_y_x + mean_y_x  # [n_variables]\n",
    "            y_x_np = y_x.detach().numpy()  # Shape: (n_variables,)\n",
    "            # Compute actual cost\n",
    "            actual_cost = 0.5 * y_x_np.T @ Q @ y_x_np  # Scalar\n",
    "            actual_costs.append(actual_cost)\n",
    "            # Compute actual constraint violations\n",
    "            inequality_violations = A @ y_x_np - b_vector  # Shape: (num_inequality_constraints,)\n",
    "            equality_violations = E @ y_x_np - d_vector    # Shape: (num_equality_constraints,)\n",
    "\n",
    "            actual_constraint = np.concatenate((inequality_violations, equality_violations))  # Shape: (num_constraints,)\n",
    "            actual_constraints.extend(actual_constraint)\n",
    "            # Get model predictions\n",
    "            # Prepare batch data for model input\n",
    "            data_obj_batch = Batch.from_data_list([data_obj])\n",
    "            data_feas_batch = Batch.from_data_list([data_feas])\n",
    "            x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "            # Denormalize predicted cost and constraints\n",
    "            predicted_cost_denorm = (predicted_cost.item() * std_y_cost + mean_y_cost)\n",
    "            predicted_constraints_denorm = (predicted_constraints * std_y_constraints + mean_y_constraints).detach().numpy()\n",
    "            predicted_costs.append(predicted_cost_denorm)\n",
    "            predicted_constraints_list.extend(predicted_constraints_denorm)  # Use the renamed list\n",
    "            \n",
    "            # print(equality_violations)\n",
    "            # print(\"Ineq:\",inequality_violations)\n",
    "            # print(predicted_constraints_denorm)\n",
    "\n",
    "    # Compute correlations\n",
    "    cost_correlation = np.corrcoef(actual_costs, predicted_costs)[0,1]\n",
    "    constraint_correlation = np.corrcoef(np.array(actual_constraints), np.array(predicted_constraints_list))[0,1]\n",
    "    print(f\"Cost Prediction Correlation: {cost_correlation}\")\n",
    "    print(f\"Constraint Prediction Correlation: {constraint_correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Prediction Correlation: 0.992529452308483\n",
      "Constraint Prediction Correlation: 0.9967090644369142\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have test_data_obj and test_data_feas\n",
    "evaluate_predictions(model, test_data_obj, test_data_feas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute the mean embedding over all training samples\n",
    "x_var_list = []\n",
    "for data_obj_batch, data_feas_batch in train_loader:\n",
    "    with torch.no_grad():\n",
    "        x_obj = model.encoder_obj(data_obj_batch)\n",
    "        x_cons_var, x_cons_constraints = model.encoder_cons(data_feas_batch)\n",
    "        x_obj_var = x_obj[data_obj_batch.variable_mask]\n",
    "        x_var = torch.cat([x_obj_var, x_cons_var], dim=1)\n",
    "        x_var_list.append(x_var)\n",
    "x_var_all = torch.cat(x_var_list, dim=0)\n",
    "mean_x_var = x_var_all.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=joint_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var_list = []\n",
    "for data_obj_batch, data_feas_batch in train_loader:\n",
    "    with torch.no_grad():\n",
    "        # Encode object and constraints\n",
    "        x_obj = model.encoder_obj(data_obj_batch)\n",
    "        x_cons_var, x_cons_constraints = model.encoder_cons(data_feas_batch)\n",
    "        \n",
    "        # Apply variable mask\n",
    "        x_obj_var = x_obj[data_obj_batch.variable_mask]\n",
    "        \n",
    "        # Concatenate object and constraint embeddings\n",
    "        x_var = torch.cat([x_obj_var, x_cons_var], dim=1)\n",
    "        x_var_list.append(x_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize accumulators for sums and counts for each variable index\n",
    "variable_sums = defaultdict(lambda: torch.zeros_like(x_var_list[0][0]))  # Sum for each variable index\n",
    "variable_counts = defaultdict(int)  # Count for each variable index\n",
    "\n",
    "# Aggregate embeddings for each variable index\n",
    "for x_var in x_var_list:\n",
    "    num_vars = x_var.size(0)  # Number of variables in this problem\n",
    "    for var_idx in range(num_vars):\n",
    "        variable_sums[var_idx] += x_var[var_idx]\n",
    "        variable_counts[var_idx] += 1\n",
    "\n",
    "# Compute mean embeddings for each variable index\n",
    "mean_x_var = {\n",
    "    var_idx: variable_sums[var_idx] / variable_counts[var_idx]\n",
    "    for var_idx in variable_sums.keys()\n",
    "}\n",
    "\n",
    "# Optionally, convert to a list for ordered output\n",
    "mean_x_var_list = [mean_x_var[var_idx] for var_idx in sorted(mean_x_var.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize x_var with a training sample\n",
    "data_obj_batch, data_feas_batch = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    x_obj = model.encoder_obj(data_obj_batch)\n",
    "    x_cons_var, _ = model.encoder_cons(data_feas_batch)\n",
    "    x_obj_var = x_obj[data_obj_batch.variable_mask]\n",
    "    initial_x_var = torch.cat([x_obj_var, x_cons_var], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 384])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Compute mean embeddings for variables and constraints\n",
    "def compute_mean_embeddings(train_loader):\n",
    "    variable_sums = defaultdict(lambda: torch.zeros(model.encoder_obj.conv2.out_channels + model.encoder_cons.conv2.out_channels))\n",
    "    variable_counts = defaultdict(int)\n",
    "    constraint_sums = defaultdict(lambda: torch.zeros(model.encoder_cons.conv2.out_channels))\n",
    "    constraint_counts = defaultdict(int)\n",
    "\n",
    "    for data_obj_batch, data_feas_batch in train_loader:\n",
    "        with torch.no_grad():\n",
    "            x_obj = model.encoder_obj(data_obj_batch)\n",
    "            x_cons_var, x_cons_constraints = model.encoder_cons(data_feas_batch)\n",
    "\n",
    "            x_obj_var = x_obj[data_obj_batch.variable_mask]\n",
    "            x_var = torch.cat([x_obj_var, x_cons_var], dim=1)\n",
    "\n",
    "            # Aggregate variable embeddings\n",
    "            for idx in range(x_var.size(0)):\n",
    "                variable_sums[idx] += x_var[idx]\n",
    "                variable_counts[idx] += 1\n",
    "\n",
    "            # Aggregate constraint embeddings\n",
    "            for idx in range(x_cons_constraints.size(0)):\n",
    "                constraint_sums[idx] += x_cons_constraints[idx]\n",
    "                constraint_counts[idx] += 1\n",
    "\n",
    "    # Compute mean embeddings\n",
    "    mean_x_var_list = [variable_sums[idx] / variable_counts[idx] for idx in sorted(variable_sums.keys())]\n",
    "    mean_x_cons_constraints_list = [constraint_sums[idx] / constraint_counts[idx] for idx in sorted(constraint_sums.keys())]\n",
    "\n",
    "    mean_x_var_tensor = torch.stack(mean_x_var_list)\n",
    "    mean_x_cons_constraints_tensor = torch.stack(mean_x_cons_constraints_list)\n",
    "\n",
    "    return mean_x_var_tensor, mean_x_cons_constraints_tensor\n",
    "\n",
    "mean_x_var_tensor, mean_x_cons_constraints_tensor = compute_mean_embeddings(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/1000, Total Loss: 319.3059, Cost Loss: 0.7640, Constraint Loss: 3.1843, Positivity Penalty: 0.0000\n",
      "Iteration 100/1000, Total Loss: 0.4860, Cost Loss: 0.2932, Constraint Loss: 0.0010, Positivity Penalty: 0.0000\n",
      "Iteration 200/1000, Total Loss: 0.3499, Cost Loss: 0.2696, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 300/1000, Total Loss: 0.2998, Cost Loss: 0.2233, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 400/1000, Total Loss: 0.2281, Cost Loss: 0.1537, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 500/1000, Total Loss: 0.1317, Cost Loss: 0.0569, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 600/1000, Total Loss: 0.0825, Cost Loss: 0.0077, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 700/1000, Total Loss: 0.0701, Cost Loss: -0.0033, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 800/1000, Total Loss: 0.0678, Cost Loss: -0.0034, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 900/1000, Total Loss: 0.0658, Cost Loss: -0.0034, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n",
      "Iteration 1000/1000, Total Loss: 0.0640, Cost Loss: -0.0034, Constraint Loss: 0.0000, Positivity Penalty: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Compute mean embeddings for variables and constraints (unchanged)\n",
    "# (Assuming mean_x_var_tensor and mean_x_cons_constraints_tensor are already computed)\n",
    "\n",
    "# Step 2: Initialize x_var and x_cons_constraints with mean embeddings (unchanged)\n",
    "x_var = mean_x_var_tensor.clone().detach().requires_grad_(True)\n",
    "x_cons_constraints = mean_x_cons_constraints_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Step 3: Set up optimizer (unchanged)\n",
    "optimizer = optim.Adam([x_var, x_cons_constraints], lr=0.01)\n",
    "\n",
    "# Hyperparameters for penalties (adjust as needed)\n",
    "alpha = 100.0  # Weight for constraint violation loss\n",
    "beta = 0.1     # Weight for variable embedding penalty\n",
    "gamma = 0.1    # Weight for constraint embedding penalty\n",
    "delta = 1000.0   # Weight for positivity penalty \n",
    "\n",
    "# Step 4: Optimization loop\n",
    "num_iterations = 1000  # Adjust as needed\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Decode to get predictions\n",
    "    x_hat = model.decoder_x(x_var).squeeze()  # x_hat shape: [n_variables]\n",
    "\n",
    "    # Compute cost loss\n",
    "    Q_tensor = torch.tensor(Q, dtype=torch.float32)\n",
    "    cost_loss = 0.5 * x_hat @ Q_tensor @ x_hat\n",
    "\n",
    "    # Compute constraint violations\n",
    "    A_tensor = torch.tensor(A, dtype=torch.float32)\n",
    "    b_tensor = torch.tensor(b_vector, dtype=torch.float32)\n",
    "    E_tensor = torch.tensor(E, dtype=torch.float32)\n",
    "    d_tensor = torch.tensor(d_vector, dtype=torch.float32)\n",
    "\n",
    "    # Inequality constraints: A x_hat <= b_vector\n",
    "    inequality_violations = A_tensor @ x_hat - b_tensor\n",
    "    inequality_constraint_loss = torch.relu(inequality_violations).pow(2).sum()\n",
    "\n",
    "    # Equality constraints: E x_hat = d_vector\n",
    "    equality_violations = E_tensor @ x_hat - d_tensor\n",
    "    equality_constraint_loss = equality_violations.pow(2).sum()\n",
    "\n",
    "    # Total constraint loss\n",
    "    constraint_loss = inequality_constraint_loss + 10 * equality_constraint_loss\n",
    "\n",
    "    # Penalty terms to prevent drifting from latent space\n",
    "    penalty_var = torch.norm(x_var - mean_x_var_tensor, p=2) ** 2\n",
    "    penalty_cons = torch.norm(x_cons_constraints - mean_x_cons_constraints_tensor, p=2) ** 2\n",
    "\n",
    "    # Positivity penalty: penalize negative values in x_hat\n",
    "    # (max(0, -x_hat)) picks out the negative parts of x_hat and squares them.\n",
    "    pos_penalty = torch.relu(-x_hat).pow(2).sum()\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = (cost_loss \n",
    "                  + alpha * constraint_loss \n",
    "                  + beta * penalty_var \n",
    "                  + gamma * penalty_cons \n",
    "                  + delta * pos_penalty)\n",
    "\n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if ((iteration + 1) % 100 == 0) or iteration == 1:\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, \"\n",
    "              f\"Total Loss: {total_loss.item():.4f}, \"\n",
    "              f\"Cost Loss: {cost_loss.item():.4f}, \"\n",
    "              f\"Constraint Loss: {constraint_loss.item():.4f}, \"\n",
    "              f\"Positivity Penalty: {pos_penalty.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Solution:\n",
      "[ 0.05208467  0.06379856  0.06121233  0.06741619  0.0555339   0.05885079\n",
      "  0.06290089  0.05955774  0.0603669   0.05895305  0.05225235  0.06625938\n",
      "  0.06140025  0.06156686  0.04990089  0.05191803  0.06172318 -0.00048151\n",
      " -0.00030778 -0.00028817 -0.00048539 -0.00046691 -0.00048443 -0.00048491\n",
      " -0.00045524 -0.0003931  -0.00047387 -0.00046572 -0.0004654  -0.00046959\n",
      "  0.05206461  0.07910225  0.06120596  0.07814809  0.05552585  0.06378697\n",
      "  0.0647719   0.05954953  0.06638554  0.05895269  0.0522446   0.08268795\n",
      "  0.06369841  0.06622885  0.04987273  0.05189208  0.08201489  0.05175669\n",
      "  0.05019291  0.0498195   0.0523144   0.07151378  0.0328604   0.06628926\n",
      "  0.04017454  0.04739257  0.0417088   0.06435848  0.09680947  0.0526416 ]\n",
      "Optimized Cost: -0.0034480527676062633\n",
      "Feasible: False\n",
      "Inequality Violations (should be <= 0): [-3.19403470e+00  2.00532377e-05 -1.53036863e-02  6.36652112e-06\n",
      " -1.07318982e-02  8.04662704e-06 -4.93617728e-03 -1.87101215e-03\n",
      "  8.21799040e-06 -6.01863489e-03  3.61353159e-07  7.75232911e-06\n",
      " -1.64285675e-02 -2.29816511e-03 -4.66199219e-03  2.81669199e-05\n",
      "  2.59503722e-05 -2.02917047e-02 -5.22382073e-02 -5.05006947e-02\n",
      " -5.01076654e-02 -5.27997911e-02 -7.19806924e-02 -3.33448239e-02\n",
      " -6.67741708e-02 -4.06297855e-02 -4.77856733e-02 -4.21826690e-02\n",
      " -6.48242012e-02 -9.72748697e-02 -5.31111918e-02]\n",
      "Equality Violations (should be close to 0): [-2.60621309e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_445/813874946.py:4: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  optimized_solution = np.array(x_hat, dtype = np.float64)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Obtain the optimized solution\n",
    "with torch.no_grad():\n",
    "    # Denormalize x_hat\n",
    "    optimized_solution = np.array(x_hat, dtype = np.float64)\n",
    "\n",
    "    # Compute actual cost and constraint violations\n",
    "    actual_cost = 0.5 * optimized_solution.T @ Q @ optimized_solution\n",
    "    inequality_violations = A @ optimized_solution - b_vector\n",
    "    equality_violations = E @ optimized_solution - d_vector\n",
    "\n",
    "    # Check feasibility\n",
    "    is_feasible = np.all(inequality_violations <= 0) and np.allclose(equality_violations, 0, atol=1e-4)\n",
    "\n",
    "    print(\"\\nOptimized Solution:\")\n",
    "    print(optimized_solution)\n",
    "    print(f\"Optimized Cost: {actual_cost}\")\n",
    "    print(f\"Feasible: {is_feasible}\")\n",
    "    print(f\"Inequality Violations (should be <= 0): {inequality_violations}\")\n",
    "    print(f\"Equality Violations (should be close to 0): {equality_violations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Compute mean embeddings for variables and constraints (unchanged)\n",
    "# (Assuming you have already computed mean_x_var_tensor and mean_x_cons_constraints_tensor)\n",
    "\n",
    "# Step 2: Initialize x_var and x_cons_constraints with mean embeddings (unchanged)\n",
    "x_var = mean_x_var_tensor.clone().detach().requires_grad_(True)\n",
    "x_cons_constraints = mean_x_cons_constraints_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Step 3: Set up optimizer (unchanged)\n",
    "optimizer = optim.Adam([x_var, x_cons_constraints], lr=0.01)\n",
    "\n",
    "# Hyperparameters for penalties (adjust as needed)\n",
    "alpha = 30000  # Weight for constraint violation loss\n",
    "beta = 0.1   # Weight for variable embedding penalty\n",
    "gamma = 0.1  # Weight for constraint embedding penalty\n",
    "\n",
    "# Step 4: Optimization loop\n",
    "num_iterations = 10000  # Adjust as needed\n",
    "for iteration in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Decode to get predictions\n",
    "    x_hat = model.decoder_x(x_var).squeeze()  # x_hat shape: [n_variables]\n",
    "    x_hat = x_hat * std_y_x + mean_y_x\n",
    "    x_hat = torch.nn.functional.softplus(x_hat)  # Ensures x_hat >= 0\n",
    "\n",
    "    # Compute the actual cost from x_hat\n",
    "    Q_tensor = torch.tensor(Q, dtype=torch.float32)\n",
    "    cost_loss = 0.5 * x_hat @ Q_tensor @ x_hat\n",
    "\n",
    "    # Compute constraint violations from x_hat\n",
    "    A_tensor = torch.tensor(A, dtype=torch.float32)\n",
    "    b_tensor = torch.tensor(b_vector, dtype=torch.float32)\n",
    "    E_tensor = torch.tensor(E, dtype=torch.float32)\n",
    "    d_tensor = torch.tensor(d_vector, dtype=torch.float32)\n",
    "\n",
    "    # Inequality constraints: A x_hat <= b_vector\n",
    "    inequality_violations = A_tensor @ x_hat - b_tensor\n",
    "    inequality_constraint_loss = torch.relu(inequality_violations).pow(2).sum()\n",
    "\n",
    "    # Equality constraints: E x_hat = d_vector\n",
    "    equality_violations = E_tensor @ x_hat - d_tensor\n",
    "    equality_constraint_loss = equality_violations.pow(2).sum()\n",
    "\n",
    "    # Total constraint loss\n",
    "    constraint_loss = inequality_constraint_loss + 10 * equality_constraint_loss\n",
    "\n",
    "    # Penalty terms to prevent drifting from the latent space\n",
    "    penalty_var = torch.norm(x_var - mean_x_var_tensor, p=2) ** 2\n",
    "    penalty_cons = torch.norm(x_cons_constraints - mean_x_cons_constraints_tensor, p=2) ** 2\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = cost_loss + alpha * constraint_loss + beta * penalty_var + gamma * penalty_cons\n",
    "\n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional: Print progress\n",
    "    if ((iteration + 1) % 100 == 0) or iteration == 1:\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Total Loss: {total_loss.item():.4f}, \"\n",
    "              f\"Cost Loss: {cost_loss.item():.4f}, Constraint Loss: {constraint_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Obtain the optimized solution\n",
    "with torch.no_grad():\n",
    "    # Denormalize x_hat\n",
    "    x_hat_denorm = x_hat \n",
    "    optimized_solution = x_hat_denorm.numpy()\n",
    "\n",
    "    # Compute actual cost and constraint violations\n",
    "    actual_cost = 0.5 * optimized_solution.T @ Q @ optimized_solution\n",
    "    inequality_violations = A @ optimized_solution - b_vector\n",
    "    equality_violations = E @ optimized_solution - d_vector\n",
    "\n",
    "    # Check feasibility\n",
    "    is_feasible = np.all(inequality_violations <= 1e-4) and np.allclose(equality_violations, 0, atol=1e-4)\n",
    "\n",
    "    print(\"\\nOptimized Solution:\")\n",
    "    print(optimized_solution)\n",
    "    print(f\"Optimized Cost: {actual_cost}\")\n",
    "    print(f\"Feasible: {is_feasible}\")\n",
    "    print(f\"Inequality Violations (should be <= 0): {inequality_violations}\")\n",
    "    print(f\"Equality Violations (should be close to 0): {equality_violations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
