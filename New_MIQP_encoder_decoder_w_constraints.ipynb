{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install gurobipy\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader  # Corrected import\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_mean_pool  # For pooling in the decoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt  # For plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_QP import my_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n",
      "Set parameter MIPGap to value 0.05\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (linux64 - \"Ubuntu 22.04.5 LTS\")\n",
      "\n",
      "CPU model: Intel Xeon Processor (Icelake), instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 16 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "MIPGap  0.05\n",
      "\n",
      "Optimize a model with 32 rows, 60 columns and 120 nonzeros\n",
      "Model fingerprint: 0x00d24133\n",
      "Model has 464 quadratic objective terms\n",
      "Variable types: 30 continuous, 30 integer (30 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [3e+01, 2e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 5e+00]\n",
      "Found heuristic solution: objective 3654.4800000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 901 rows, 526 columns, 2321 nonzeros\n",
      "Presolved model has 30 quadratic constraint(s)\n",
      "Presolved model has 434 bilinear constraint(s)\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 496 continuous, 30 integer (30 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 1 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                      31.9607000    0.00000   100%     -    0s\n",
      "*    0     0               0      31.9607000    0.00000   100%     -    0s\n",
      "H    0     0                      20.8607250    0.00000   100%     -    0s\n",
      "*    0     0               0      20.8607250    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    2   20.86072    0.00000   100%     -    0s\n",
      "H    0     0                      18.2729563    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "H    0     0                      17.6415556    0.00000   100%     -    0s\n",
      "H    0     0                      17.4674945    0.00000   100%     -    0s\n",
      "H    0     0                      17.4402731    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   10   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.08623    0  149   17.44027    0.08623   100%     -    0s\n",
      "     0     0    1.81173    0   10   17.44027    1.81173  89.6%     -    0s\n",
      "     0     0    1.81174    0   17   17.44027    1.81174  89.6%     -    0s\n",
      "     0     0    2.62598    0   33   17.44027    2.62598  84.9%     -    0s\n",
      "     0     0    3.56723    0   25   17.44027    3.56723  79.5%     -    0s\n",
      "     0     0    3.93799    0   10   17.44027    3.93799  77.4%     -    0s\n",
      "     0     0    4.38555    0   22   17.44027    4.38555  74.9%     -    0s\n",
      "     0     0    5.55565    0   22   17.44027    5.55565  68.1%     -    0s\n",
      "     0     0    7.15470    0   27   17.44027    7.15470  59.0%     -    0s\n",
      "     0     0    7.69619    0   20   17.44027    7.69619  55.9%     -    0s\n",
      "     0     0    7.73872    0   29   17.44027    7.73872  55.6%     -    0s\n",
      "     0     0    7.78685    0   29   17.44027    7.78685  55.4%     -    0s\n",
      "     0     0    7.79280    0   29   17.44027    7.79280  55.3%     -    0s\n",
      "     0     0    7.80101    0   29   17.44027    7.80101  55.3%     -    0s\n",
      "H    0     0                      16.3118000    7.80101  52.2%     -    0s\n",
      "     0     0    7.80101    0   29   16.31180    7.80101  52.2%     -    0s\n",
      "H    0     0                      16.2628544    7.80101  52.0%     -    0s\n",
      "H    0     0                      15.9880975    7.80101  51.2%     -    0s\n",
      "H    0     0                      15.7081129    7.80101  50.3%     -    0s\n",
      "     0     2    7.80101    0   29   15.70811    7.80101  50.3%     -    0s\n",
      "H 1390  1093                      15.5444898    8.44228  45.7%  11.6    0s\n",
      "H 1390  1093                      15.5368906    8.44228  45.7%  11.6    0s\n",
      "H 1390  1093                      15.5357286    8.44228  45.7%  11.6    0s\n",
      "H 1390  1091                      15.5339589    8.44228  45.7%  11.6    0s\n",
      "H 1393  1035                      15.5143201    8.44228  45.6%  11.6    0s\n",
      "H 1393   983                      15.5114375    8.44228  45.6%  11.6    0s\n",
      "H 1393   934                      15.5114213    8.44228  45.6%  11.6    0s\n",
      "H 1393   887                      15.5114095    8.44228  45.6%  11.6    0s\n",
      "H 1393   843                      15.5112715    8.44228  45.6%  11.6    0s\n",
      "H 1393   800                      15.5112351    8.44228  45.6%  11.6    0s\n",
      "H 1394   761                      15.4825660    8.44228  45.5%  11.6    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Flow cover: 7\n",
      "  RLT: 16\n",
      "  Relax-and-lift: 4\n",
      "  PSD: 235\n",
      "\n",
      "Explored 1403 nodes (17911 simplex iterations) in 1.24 seconds (1.03 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 15.4826 15.5112 15.5113 ... 15.5369\n",
      "\n",
      "Optimal solution found (tolerance 5.00e-02)\n",
      "Best objective 1.548256597506e+01, best bound 1.470906615643e+01, gap 4.9959%\n",
      "\n",
      "User-callback calls 3732, time in user-callback 0.03 sec\n"
     ]
    }
   ],
   "source": [
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "# Solution storage\n",
    "grb_model._feasible_solutions = []\n",
    "grb_model._relaxation_solutions = []\n",
    "grb_model.setParam(\"MIPGap\", 0.05)\n",
    "#model.setParam(\"NodeLimit\", 100)  # Explore a limited number of nodes\n",
    "\n",
    "# Optimize\n",
    "grb_model.optimize(my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.0, 0.0, 0.3328307282391785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1574644983911741, 0.25564451699144053, 0.0, 0.16105507040036132, 0.0, 0.0, 0.09300518597785103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 1.0, 0.0, -0.0, -0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, -0.0, 0.0, 1.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve optimal solution if available\n",
    "if grb_model.status == GRB.OPTIMAL:\n",
    "    optimal_solution = grb_model.getAttr('X', grb_model.getVars())\n",
    "    print(\"Optimal solution:\", optimal_solution)\n",
    "else:\n",
    "    print(f\"Model status: {grb_model.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n"
     ]
    }
   ],
   "source": [
    "## Extracting bounds\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "variable_bounds = {}\n",
    "for var in grb_model.getVars():\n",
    "    variable_bounds[var.VarName] = {'Lower': var.LB, 'Upper': var.UB}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Q, A, b, d, etc\n",
    "\n",
    "from parse_QP import parse_qplib_file\n",
    "\n",
    "# Replace '0031' with the desired file number\n",
    "data = parse_qplib_file('0031')\n",
    "\n",
    "# Access the data\n",
    "A = data['A']\n",
    "b_vector = data['b_vector']\n",
    "E = data['E']\n",
    "d_vector = data['d']\n",
    "Q = data['Q']\n",
    "variables_info = data['variables_info']\n",
    "variables_info = [v[0] for v in variables_info]\n",
    "binary_indices = data['binary_indices']\n",
    "variable_indices = data['variable_indices']\n",
    "\n",
    "m, n = A.shape\n",
    "# Get indices of non-zero elements in A\n",
    "row_indices, col_indices = np.nonzero(A)\n",
    "edge_weights = A[row_indices, col_indices]\n",
    "\n",
    "# Map variable types to numerical values\n",
    "# Node types: 0 - continuous, 1 - binary\n",
    "variable_types = np.array([0 if v[0] == 'x' else 1 for v in variables_info])\n",
    "\n",
    "# Collect indices of continuous and binary variables\n",
    "continuous_indices = np.where(variable_types == 0)[0]\n",
    "binary_indices = np.where(variable_types == 1)[0]\n",
    "n_continuous = len(continuous_indices)\n",
    "n_binary = len(binary_indices)\n",
    "n_variables = n_continuous + n_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing feasible solutions from file.\n",
      "Loaded existing infeasible solutions from file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from generate_solutions import generate_feasible_solutions, generate_infeasible_solutions\n",
    "\n",
    "# Set generate_new to False by default\n",
    "generate_new = False\n",
    "feasible_data_file = 'feasible_data.pkl'\n",
    "infeasible_data_file = 'infeasible_data.pkl'\n",
    "num_samples = 5000\n",
    "\n",
    "# Load or generate feasible solutions (as in your original code)\n",
    "if not generate_new and os.path.exists(feasible_data_file):\n",
    "    # Load feasible data\n",
    "    with open(feasible_data_file, 'rb') as f:\n",
    "        feasible_data = pickle.load(f)\n",
    "    feasible_solutions = feasible_data['solutions']\n",
    "    feasible_costs = feasible_data['costs']\n",
    "    print(\"Loaded existing feasible solutions from file.\")\n",
    "else:\n",
    "    # Generate feasible solutions\n",
    "    num_objectives = num_samples  # Adjust the number as needed\n",
    "    feasible_solutions, feasible_costs = generate_feasible_solutions(\n",
    "        A, E, Q, variables_info, b_vector, d_vector, num_objectives\n",
    "    )\n",
    "    # Save the generated data for future use\n",
    "    feasible_data = {'solutions': feasible_solutions, 'costs': feasible_costs}\n",
    "    with open(feasible_data_file, 'wb') as f:\n",
    "        pickle.dump(feasible_data, f)\n",
    "    print(\"Generated and saved feasible solutions.\")\n",
    "\n",
    "# Now, load or generate infeasible solutions\n",
    "if not generate_new and os.path.exists(infeasible_data_file):\n",
    "    # Load infeasible data\n",
    "    with open(infeasible_data_file, 'rb') as f:\n",
    "        infeasible_data = pickle.load(f)\n",
    "    infeasible_solutions = infeasible_data['solutions']\n",
    "    infeasible_costs = infeasible_data['costs']\n",
    "    print(\"Loaded existing infeasible solutions from file.\")\n",
    "else:\n",
    "    # Generate infeasible solutions\n",
    "    num_infeasible_samples = num_samples  # Adjust as needed\n",
    "    infeasible_solutions, infeasible_costs = generate_infeasible_solutions(\n",
    "        A, E, variables_info, b_vector, d_vector, Q, num_infeasible_samples, feasible_solutions\n",
    "    )\n",
    "    # Save the generated data for future use\n",
    "    infeasible_data = {'solutions': infeasible_solutions, 'costs': infeasible_costs}\n",
    "    with open(infeasible_data_file, 'wb') as f:\n",
    "        pickle.dump(infeasible_data, f)\n",
    "    print(\"Generated and saved infeasible solutions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Test Loss: 2.4887, Recon Loss: 1.0111, Cost Loss: 0.5489, Constraint Loss: 0.9286\n",
      "----\n",
      "Epoch 1, Train Loss: 0.7501, Recon Loss: 0.0765, Cost Loss: 0.3620, Constraint Loss: 0.3116\n",
      "Test Loss: 0.0926, Recon Loss: 0.0001, Cost Loss: 0.0649, Constraint Loss: 0.0277\n",
      "----\n",
      "----\n",
      "Epoch 2, Train Loss: 0.1445, Recon Loss: 0.0002, Cost Loss: 0.1373, Constraint Loss: 0.0069\n",
      "Test Loss: 0.0940, Recon Loss: 0.0000, Cost Loss: 0.0699, Constraint Loss: 0.0241\n",
      "----\n",
      "----\n",
      "Epoch 3, Train Loss: 0.1177, Recon Loss: 0.0002, Cost Loss: 0.1126, Constraint Loss: 0.0050\n",
      "Test Loss: 0.0660, Recon Loss: 0.0000, Cost Loss: 0.0427, Constraint Loss: 0.0232\n",
      "----\n",
      "----\n",
      "Epoch 4, Train Loss: 0.0987, Recon Loss: 0.0001, Cost Loss: 0.0945, Constraint Loss: 0.0041\n",
      "Test Loss: 0.0515, Recon Loss: 0.0000, Cost Loss: 0.0286, Constraint Loss: 0.0229\n",
      "----\n",
      "----\n",
      "Epoch 5, Train Loss: 0.0877, Recon Loss: 0.0001, Cost Loss: 0.0839, Constraint Loss: 0.0037\n",
      "Test Loss: 0.0637, Recon Loss: 0.0000, Cost Loss: 0.0420, Constraint Loss: 0.0216\n",
      "----\n",
      "----\n",
      "Epoch 6, Train Loss: 0.0647, Recon Loss: 0.0001, Cost Loss: 0.0609, Constraint Loss: 0.0038\n",
      "Test Loss: 0.0443, Recon Loss: 0.0000, Cost Loss: 0.0225, Constraint Loss: 0.0217\n",
      "----\n",
      "----\n",
      "Epoch 7, Train Loss: 0.0466, Recon Loss: 0.0001, Cost Loss: 0.0421, Constraint Loss: 0.0044\n",
      "Test Loss: 0.0405, Recon Loss: 0.0001, Cost Loss: 0.0182, Constraint Loss: 0.0223\n",
      "----\n",
      "----\n",
      "Epoch 8, Train Loss: 0.0310, Recon Loss: 0.0001, Cost Loss: 0.0267, Constraint Loss: 0.0042\n",
      "Test Loss: 0.0305, Recon Loss: 0.0000, Cost Loss: 0.0070, Constraint Loss: 0.0234\n",
      "----\n",
      "----\n",
      "Epoch 9, Train Loss: 0.0232, Recon Loss: 0.0002, Cost Loss: 0.0180, Constraint Loss: 0.0050\n",
      "Test Loss: 0.0296, Recon Loss: 0.0002, Cost Loss: 0.0037, Constraint Loss: 0.0257\n",
      "----\n",
      "----\n",
      "Epoch 10, Train Loss: 0.0169, Recon Loss: 0.0003, Cost Loss: 0.0116, Constraint Loss: 0.0049\n",
      "Test Loss: 0.0287, Recon Loss: 0.0001, Cost Loss: 0.0025, Constraint Loss: 0.0261\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import numpy as np\n",
    "\n",
    "# Adjusted GNNModelObj to output variable node embeddings\n",
    "class GNNModelObj(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNNModelObj, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))\n",
    "        return x  # Return embeddings for all variable nodes\n",
    "\n",
    "# Adjusted GNNModelConstraints to output variable and constraint node embeddings\n",
    "class GNNModelConstraints(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNNModelConstraints, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))\n",
    "        x_var = x[data.variable_mask]\n",
    "        x_constraints = x[~data.variable_mask]\n",
    "        return x_var, x_constraints  # Return embeddings for both variables and constraints\n",
    "\n",
    "# Define the JointGNN model\n",
    "class JointGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels_obj, hidden_channels_cons, decoder_hidden_channels):\n",
    "        super(JointGNN, self).__init__()\n",
    "        # Encoders\n",
    "        self.encoder_obj = GNNModelObj(hidden_channels_obj)\n",
    "        self.encoder_cons = GNNModelConstraints(hidden_channels_cons)\n",
    "        # Decoder for x reconstruction\n",
    "        concat_dim = hidden_channels_obj + hidden_channels_cons\n",
    "        self.decoder_x = torch.nn.Sequential(\n",
    "            torch.nn.Linear(concat_dim, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "        # Decoder for cost prediction\n",
    "        self.decoder_cost = torch.nn.Sequential(\n",
    "            torch.nn.Linear(concat_dim, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "        # Decoder for constraint violation prediction\n",
    "        self.decoder_constraints = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels_cons, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_obj, data_feas):\n",
    "        # Process data through encoders\n",
    "        x_obj = self.encoder_obj(data_obj)  # Embeddings for variables\n",
    "        x_cons_var, x_cons_constraints = self.encoder_cons(data_feas)  # Embeddings for variables and constraints\n",
    "        # Extract variable embeddings\n",
    "        x_obj_var = x_obj[data_obj.variable_mask]\n",
    "        # Concatenate per-variable embeddings\n",
    "        x_var = torch.cat([x_obj_var, x_cons_var], dim=1)\n",
    "        # Decode to reconstruct x\n",
    "        x_hat = self.decoder_x(x_var).squeeze()\n",
    "        # Compute mean pooling over variable nodes for cost prediction\n",
    "        batch = data_obj.batch[data_obj.variable_mask]\n",
    "        x_var_pooled = global_mean_pool(x_var, batch)  # [batch_size, concat_dim]\n",
    "        # Predict cost\n",
    "        predicted_cost = self.decoder_cost(x_var_pooled).squeeze()\n",
    "        # Decode to predict constraint violations\n",
    "        predicted_constraints = self.decoder_constraints(x_cons_constraints).squeeze()\n",
    "        return x_hat, predicted_cost, predicted_constraints\n",
    "\n",
    "# Custom dataset to return pairs of data_obj and data_feas\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, data_list_obj, data_list_feas):\n",
    "        assert len(data_list_obj) == len(data_list_feas)\n",
    "        self.data_list_obj = data_list_obj\n",
    "        self.data_list_feas = data_list_feas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_obj)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list_obj[idx], self.data_list_feas[idx]\n",
    "\n",
    "# Custom collate function for batching\n",
    "def joint_collate_fn(batch):\n",
    "    data_obj_list, data_feas_list = zip(*batch)\n",
    "    batch_obj = Batch.from_data_list(data_obj_list)\n",
    "    batch_feas = Batch.from_data_list(data_feas_list)\n",
    "    return batch_obj, batch_feas\n",
    "\n",
    "# Data preparation functions\n",
    "def prepare_edge_index_and_attr(Q):\n",
    "    n_variables = Q.shape[0]\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    Q = Q if isinstance(Q, np.ndarray) else Q.toarray()\n",
    "    for i in range(n_variables):\n",
    "        for j in range(n_variables):\n",
    "            if Q[i, j] != 0:\n",
    "                edge_index.append([i, j])\n",
    "                edge_attr.append(Q[i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def prepare_constraint_edge_data(A, E, n_variables):\n",
    "    # A: m x n, E: p x n\n",
    "    m = A.shape[0]\n",
    "    p = E.shape[0]\n",
    "    num_constraints = m + p\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    # Edges from variables to inequality constraints\n",
    "    for constraint_idx in range(m):\n",
    "        for variable_idx in range(n_variables):\n",
    "            coeff = A[constraint_idx, variable_idx]\n",
    "            if coeff != 0:\n",
    "                # Edge from variable to constraint\n",
    "                edge_index.append([variable_idx, n_variables + constraint_idx])\n",
    "                edge_attr.append(coeff)\n",
    "    # Edges from variables to equality constraints\n",
    "    for constraint_idx in range(p):\n",
    "        for variable_idx in range(n_variables):\n",
    "            coeff = E[constraint_idx, variable_idx]\n",
    "            if coeff != 0:\n",
    "                # Edge from variable to constraint\n",
    "                edge_index.append([variable_idx, n_variables + m + constraint_idx])\n",
    "                edge_attr.append(coeff)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def create_data_list_obj(solutions, costs, edge_index, edge_attr):\n",
    "    data_list = []\n",
    "    for x_sol, cost_sol in zip(solutions, costs):\n",
    "        x = torch.tensor(x_sol, dtype=torch.float).unsqueeze(-1)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.variable_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        data.y_x = x.squeeze()\n",
    "        data.y_cost = torch.tensor([cost_sol], dtype=torch.float)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def create_data_list_feas(solutions, costs, A, E, b_vector, d_vector, edge_index, edge_attr, n_variables):\n",
    "    data_list = []\n",
    "    m = A.shape[0]\n",
    "    p = E.shape[0]\n",
    "    num_constraints = m + p\n",
    "    for x_sol, cost_sol in zip(solutions, costs):\n",
    "        x_vars = torch.tensor(x_sol, dtype=torch.float).unsqueeze(-1)\n",
    "        # Constraint node features: b_vector and d_vector\n",
    "        b = torch.tensor(b_vector, dtype=torch.float).unsqueeze(-1)\n",
    "        d = torch.tensor(d_vector, dtype=torch.float).unsqueeze(-1)\n",
    "        x_constraints = torch.cat([b, d], dim=0)\n",
    "        x_total = torch.cat([x_vars, x_constraints], dim=0)\n",
    "        data = Data(x=x_total, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.variable_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.variable_mask[:n_variables] = True  # Variables are first\n",
    "        data.y_x = x_vars.squeeze()\n",
    "        data.y_cost = torch.tensor([cost_sol], dtype=torch.float)\n",
    "\n",
    "        # Compute constraint violations\n",
    "        A_tensor = torch.tensor(A, dtype=torch.float)\n",
    "        E_tensor = torch.tensor(E, dtype=torch.float)\n",
    "        inequality_violations = torch.mv(A_tensor, x_vars.squeeze()) - torch.tensor(b_vector, dtype=torch.float)\n",
    "        equality_violations = torch.mv(E_tensor, x_vars.squeeze()) - torch.tensor(d_vector, dtype=torch.float)\n",
    "        y_constraints = torch.cat([inequality_violations, equality_violations], dim=0)\n",
    "        data.y_constraints = y_constraints  # [num_constraints]\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def normalize_node_features(data_list):\n",
    "    # Concatenate all node features from all graphs\n",
    "    x_all = torch.cat([data.x for data in data_list], dim=0)\n",
    "    mean = x_all.mean(dim=0)\n",
    "    std = x_all.std(dim=0) + 1e-6  # Avoid division by zero\n",
    "    for data in data_list:\n",
    "        data.x = (data.x - mean) / std\n",
    "    return data_list, mean, std\n",
    "\n",
    "def normalize_targets(data_list):\n",
    "    # Normalize y_x (node targets)\n",
    "    y_x_all = torch.cat([data.y_x for data in data_list], dim=0)\n",
    "    mean_y_x = y_x_all.mean()\n",
    "    std_y_x = y_x_all.std() + 1e-6\n",
    "    for data in data_list:\n",
    "        data.y_x = (data.y_x - mean_y_x) / std_y_x\n",
    "\n",
    "    # Normalize y_cost (graph targets)\n",
    "    y_cost_all = torch.cat([data.y_cost for data in data_list], dim=0)\n",
    "    mean_y_cost = y_cost_all.mean()\n",
    "    std_y_cost = y_cost_all.std() + 1e-6\n",
    "    for data in data_list:\n",
    "        data.y_cost = (data.y_cost - mean_y_cost) / std_y_cost\n",
    "\n",
    "    # Initialize means and stds for y_constraints\n",
    "    mean_y_constraints = None\n",
    "    std_y_constraints = None\n",
    "\n",
    "    # Normalize y_constraints if present\n",
    "    if hasattr(data_list[0], 'y_constraints'):\n",
    "        y_constraints_all = torch.cat([data.y_constraints for data in data_list], dim=0)\n",
    "        mean_y_constraints = y_constraints_all.mean()\n",
    "        std_y_constraints = y_constraints_all.std() + 1e-6\n",
    "        for data in data_list:\n",
    "            data.y_constraints = (data.y_constraints - mean_y_constraints) / std_y_constraints\n",
    "\n",
    "    return data_list, (mean_y_x, std_y_x), (mean_y_cost, std_y_cost), (mean_y_constraints, std_y_constraints)\n",
    "\n",
    "def split_data(data_list, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(data_list))\n",
    "    test_set_size = int(len(data_list) * test_size)\n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "    train_data = [data_list[i] for i in train_indices]\n",
    "    test_data = [data_list[i] for i in test_indices]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Assume you have the following variables defined:\n",
    "# Q, A, E, b_vector, d_vector, feasible_solutions, feasible_costs, infeasible_solutions, infeasible_costs, n_variables\n",
    "\n",
    "# Prepare edge information from Q\n",
    "edge_index_obj, edge_attr_obj = prepare_edge_index_and_attr(Q)\n",
    "\n",
    "# Prepare edge information from A and E\n",
    "edge_index_feas, edge_attr_feas = prepare_constraint_edge_data(A, E, n_variables)\n",
    "\n",
    "# Create Data objects for feasible solutions\n",
    "data_list_obj_feas = create_data_list_obj(feasible_solutions, feasible_costs, edge_index_obj, edge_attr_obj)\n",
    "data_list_feas_feas = create_data_list_feas(feasible_solutions, feasible_costs, A, E, b_vector, d_vector, edge_index_feas, edge_attr_feas, n_variables)\n",
    "\n",
    "# Create Data objects for infeasible solutions\n",
    "data_list_obj_infeas = create_data_list_obj(infeasible_solutions, infeasible_costs, edge_index_obj, edge_attr_obj)\n",
    "data_list_feas_infeas = create_data_list_feas(infeasible_solutions, infeasible_costs, A, E, b_vector, d_vector, edge_index_feas, edge_attr_feas, n_variables)\n",
    "\n",
    "# Combine feasible and infeasible data\n",
    "data_list_obj = data_list_obj_feas + data_list_obj_infeas\n",
    "data_list_feas = data_list_feas_feas + data_list_feas_infeas\n",
    "\n",
    "# Normalize node features for data_list_obj\n",
    "data_list_obj, mean_obj, std_obj = normalize_node_features(data_list_obj)\n",
    "\n",
    "# Normalize node features for data_list_feas\n",
    "data_list_feas, mean_feas, std_feas = normalize_node_features(data_list_feas)\n",
    "\n",
    "# Normalize targets (x and cost) for data_list_obj\n",
    "data_list_obj, (mean_y_x, std_y_x), (mean_y_cost, std_y_cost), _ = normalize_targets(data_list_obj)\n",
    "\n",
    "# Normalize targets (x, cost, and constraints) for data_list_feas\n",
    "data_list_feas, _, _, (mean_y_constraints, std_y_constraints) = normalize_targets(data_list_feas)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data_obj, test_data_obj = split_data(data_list_obj, test_size=0.2, random_state=42)\n",
    "train_data_feas, test_data_feas = split_data(data_list_feas, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = JointDataset(train_data_obj, train_data_feas)\n",
    "test_dataset = JointDataset(test_data_obj, test_data_feas)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=joint_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=joint_collate_fn)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = JointGNN(hidden_channels_obj=256, hidden_channels_cons=128, decoder_hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    total_constraint_loss = 0\n",
    "    total_original_cost_loss = 0\n",
    "    total_original_constraint_loss = 0\n",
    "    for data_obj_batch, data_feas_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "        # Get target x values and cost values\n",
    "        y_x = data_obj_batch.y_x\n",
    "        y_cost = data_obj_batch.y_cost.squeeze()\n",
    "        y_constraints = data_feas_batch.y_constraints\n",
    "        # Compute reconstruction loss\n",
    "        recon_loss = criterion(x_hat, y_x)\n",
    "        # Compute cost prediction loss\n",
    "        cost_loss = criterion(predicted_cost, y_cost)\n",
    "        # Compute constraint violation loss\n",
    "        constraint_loss = criterion(predicted_constraints, y_constraints)\n",
    "        # Total loss (weighted sum)\n",
    "        loss = recon_loss + cost_loss + constraint_loss  # You can adjust weights if desired\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data_obj_batch.num_graphs\n",
    "        total_recon_loss += recon_loss.item() * data_obj_batch.num_graphs\n",
    "        total_cost_loss += cost_loss.item() * data_obj_batch.num_graphs\n",
    "        total_constraint_loss += constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "        # Compute original cost loss (denormalized)\n",
    "        predicted_cost_orig = predicted_cost * std_y_cost + mean_y_cost\n",
    "        y_cost_orig = y_cost * std_y_cost + mean_y_cost\n",
    "        original_cost_loss = criterion(predicted_cost_orig, y_cost_orig)\n",
    "        total_original_cost_loss += original_cost_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "        # Compute original constraint loss (denormalized)\n",
    "        predicted_constraints_orig = predicted_constraints * std_y_constraints + mean_y_constraints\n",
    "        y_constraints_orig = y_constraints * std_y_constraints + mean_y_constraints\n",
    "        original_constraint_loss = criterion(predicted_constraints_orig, y_constraints_orig)\n",
    "        total_original_constraint_loss += original_constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_cost_loss = total_cost_loss / len(train_loader.dataset)\n",
    "    avg_constraint_loss = total_constraint_loss / len(train_loader.dataset)\n",
    "    avg_original_cost_loss = total_original_cost_loss / len(train_loader.dataset)\n",
    "    avg_original_constraint_loss = total_original_constraint_loss / len(train_loader.dataset)\n",
    "    return avg_loss, avg_recon_loss, avg_cost_loss, avg_constraint_loss, avg_original_cost_loss, avg_original_constraint_loss\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    total_constraint_loss = 0\n",
    "    total_original_cost_loss = 0\n",
    "    total_original_constraint_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data_obj_batch, data_feas_batch in loader:\n",
    "            x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "            y_x = data_obj_batch.y_x\n",
    "            y_cost = data_obj_batch.y_cost.squeeze()\n",
    "            y_constraints = data_feas_batch.y_constraints\n",
    "            recon_loss = criterion(x_hat, y_x)\n",
    "            cost_loss = criterion(predicted_cost, y_cost)\n",
    "            constraint_loss = criterion(predicted_constraints, y_constraints)\n",
    "            loss = recon_loss + cost_loss + constraint_loss\n",
    "            total_loss += loss.item() * data_obj_batch.num_graphs\n",
    "            total_recon_loss += recon_loss.item() * data_obj_batch.num_graphs\n",
    "            total_cost_loss += cost_loss.item() * data_obj_batch.num_graphs\n",
    "            total_constraint_loss += constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "            # Compute original cost loss (denormalized)\n",
    "            predicted_cost_orig = predicted_cost * std_y_cost + mean_y_cost\n",
    "            y_cost_orig = y_cost * std_y_cost + mean_y_cost\n",
    "            original_cost_loss = criterion(predicted_cost_orig, y_cost_orig)\n",
    "            total_original_cost_loss += original_cost_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "            # Compute original constraint loss (denormalized)\n",
    "            predicted_constraints_orig = predicted_constraints * std_y_constraints + mean_y_constraints\n",
    "            y_constraints_orig = y_constraints * std_y_constraints + mean_y_constraints\n",
    "            original_constraint_loss = criterion(predicted_constraints_orig, y_constraints_orig)\n",
    "            total_original_constraint_loss += original_constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(loader.dataset)\n",
    "    avg_cost_loss = total_cost_loss / len(loader.dataset)\n",
    "    avg_constraint_loss = total_constraint_loss / len(loader.dataset)\n",
    "    avg_original_cost_loss = total_original_cost_loss / len(loader.dataset)\n",
    "    avg_original_constraint_loss = total_original_constraint_loss / len(loader.dataset)\n",
    "    return avg_loss, avg_recon_loss, avg_cost_loss, avg_constraint_loss, avg_original_cost_loss, avg_original_constraint_loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "test_loss, test_recon_loss, test_cost_loss, test_constraint_loss, test_orig_cost_loss, test_orig_constraint_loss = test(test_loader)\n",
    "\n",
    "print(f\"Pre-Test Loss: {test_loss:.4f}, Recon Loss: {test_recon_loss:.4f}, \"\n",
    "      f\"Cost Loss: {test_cost_loss:.4f}, Constraint Loss: {test_constraint_loss:.4f}\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Unpack all six returned values\n",
    "    train_loss, train_recon_loss, train_cost_loss, train_constraint_loss, train_orig_cost_loss, train_orig_constraint_loss = train()\n",
    "    test_loss, test_recon_loss, test_cost_loss, test_constraint_loss, test_orig_cost_loss, test_orig_constraint_loss = test(test_loader)\n",
    "\n",
    "    print(\"----\")\n",
    "    # Print out all the losses\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Recon Loss: {train_recon_loss:.4f}, \"\n",
    "          f\"Cost Loss: {train_cost_loss:.4f}, Constraint Loss: {train_constraint_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Recon Loss: {test_recon_loss:.4f}, \"\n",
    "          f\"Cost Loss: {test_cost_loss:.4f}, Constraint Loss: {test_constraint_loss:.4f}\")\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Reconstruction Error: 0.061145491898059845\n"
     ]
    }
   ],
   "source": [
    "# Use a batch of data\n",
    "data_obj_batch, data_feas_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    # Encode and decode\n",
    "    z_obj = model.encoder_obj(data_obj_batch)\n",
    "    z_cons_var, z_constraints = model.encoder_cons(data_feas_batch)\n",
    "    x_obj_var = z_obj[data_obj_batch.variable_mask]\n",
    "    x_var = torch.cat([x_obj_var, z_cons_var], dim=1)\n",
    "    x_hat = model.decoder_x(x_var).squeeze()\n",
    "    # Denormalize x_hat and y_x\n",
    "    x_hat_denorm = x_hat * std_y_x + mean_y_x\n",
    "    y_x_denorm = data_obj_batch.y_x * std_y_x + mean_y_x\n",
    "    # Compute reconstruction error\n",
    "    recon_error = torch.norm(x_hat_denorm - y_x_denorm)\n",
    "    print(f\"Decoder Reconstruction Error: {recon_error.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7b69a3536e50>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_x_denorm.cpu().numpy(), np.round(x_hat_denorm, 3).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(model, data_list_obj, data_list_feas):\n",
    "    model.eval()\n",
    "    actual_costs = []\n",
    "    predicted_costs = []\n",
    "    actual_constraints = []\n",
    "    predicted_constraints_list = []  # Renamed the list\n",
    "    with torch.no_grad():\n",
    "        for data_obj, data_feas in zip(data_list_obj, data_list_feas):\n",
    "            # Get the true x\n",
    "            y_x = data_obj.y_x * std_y_x + mean_y_x  # [n_variables]\n",
    "            y_x_np = y_x.detach().numpy()  # Shape: (n_variables,)\n",
    "            # Compute actual cost\n",
    "            actual_cost = 0.5 * y_x_np.T @ Q @ y_x_np  # Scalar\n",
    "            actual_costs.append(actual_cost)\n",
    "            # Compute actual constraint violations\n",
    "            inequality_violations = A @ y_x_np - b_vector  # Shape: (num_inequality_constraints,)\n",
    "            equality_violations = E @ y_x_np - d_vector    # Shape: (num_equality_constraints,)\n",
    "            actual_constraint = np.concatenate((inequality_violations, equality_violations))  # Shape: (num_constraints,)\n",
    "            actual_constraints.extend(actual_constraint)\n",
    "            # Get model predictions\n",
    "            # Prepare batch data for model input\n",
    "            data_obj_batch = Batch.from_data_list([data_obj])\n",
    "            data_feas_batch = Batch.from_data_list([data_feas])\n",
    "            x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "            # Denormalize predicted cost and constraints\n",
    "            predicted_cost_denorm = (predicted_cost.item() * std_y_cost + mean_y_cost)\n",
    "            predicted_constraints_denorm = (predicted_constraints * std_y_constraints + mean_y_constraints).detach().numpy()\n",
    "            predicted_costs.append(predicted_cost_denorm)\n",
    "            predicted_constraints_list.extend(predicted_constraints_denorm)  # Use the renamed list\n",
    "    # Compute correlations\n",
    "    cost_correlation = np.corrcoef(actual_costs, predicted_costs)[0,1]\n",
    "    constraint_correlation = np.corrcoef(np.array(actual_constraints), np.array(predicted_constraints_list))[0,1]\n",
    "    print(f\"Cost Prediction Correlation: {cost_correlation}\")\n",
    "    print(f\"Constraint Prediction Correlation: {constraint_correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Prediction Correlation: 0.9901578312283295\n",
      "Constraint Prediction Correlation: 0.9964371730046403\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have test_data_obj and test_data_feas\n",
    "evaluate_predictions(model, test_data_obj, test_data_feas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
