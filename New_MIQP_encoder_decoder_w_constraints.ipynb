{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install gurobipy\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader  # Corrected import\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_mean_pool  # For pooling in the decoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt  # For plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_QP import my_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n",
      "Set parameter MIPGap to value 0.05\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (linux64 - \"Ubuntu 22.04.5 LTS\")\n",
      "\n",
      "CPU model: Intel Xeon Processor (Icelake), instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 16 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Non-default parameters:\n",
      "MIPGap  0.05\n",
      "\n",
      "Optimize a model with 32 rows, 60 columns and 120 nonzeros\n",
      "Model fingerprint: 0x00d24133\n",
      "Model has 464 quadratic objective terms\n",
      "Variable types: 30 continuous, 30 integer (30 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [3e+01, 2e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 5e+00]\n",
      "Found heuristic solution: objective 3654.4800000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 901 rows, 526 columns, 2321 nonzeros\n",
      "Presolved model has 30 quadratic constraint(s)\n",
      "Presolved model has 434 bilinear constraint(s)\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 496 continuous, 30 integer (30 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 1 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                      31.9607000    0.00000   100%     -    0s\n",
      "*    0     0               0      31.9607000    0.00000   100%     -    0s\n",
      "H    0     0                      20.8607250    0.00000   100%     -    0s\n",
      "*    0     0               0      20.8607250    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    2   20.86072    0.00000   100%     -    0s\n",
      "H    0     0                      18.2729563    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "H    0     0                      17.6415556    0.00000   100%     -    0s\n",
      "H    0     0                      17.4674945    0.00000   100%     -    0s\n",
      "H    0     0                      17.4402731    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   10   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.08623    0  149   17.44027    0.08623   100%     -    0s\n",
      "     0     0    1.81173    0   10   17.44027    1.81173  89.6%     -    0s\n",
      "     0     0    1.81174    0   17   17.44027    1.81174  89.6%     -    0s\n",
      "     0     0    2.62598    0   33   17.44027    2.62598  84.9%     -    0s\n",
      "     0     0    3.56723    0   25   17.44027    3.56723  79.5%     -    0s\n",
      "     0     0    3.93799    0   10   17.44027    3.93799  77.4%     -    0s\n",
      "     0     0    4.38555    0   22   17.44027    4.38555  74.9%     -    0s\n",
      "     0     0    5.55565    0   22   17.44027    5.55565  68.1%     -    0s\n",
      "     0     0    7.15470    0   27   17.44027    7.15470  59.0%     -    0s\n",
      "     0     0    7.69619    0   20   17.44027    7.69619  55.9%     -    0s\n",
      "     0     0    7.73872    0   29   17.44027    7.73872  55.6%     -    0s\n",
      "     0     0    7.78685    0   29   17.44027    7.78685  55.4%     -    0s\n",
      "     0     0    7.79280    0   29   17.44027    7.79280  55.3%     -    0s\n",
      "     0     0    7.80101    0   29   17.44027    7.80101  55.3%     -    0s\n",
      "H    0     0                      16.3118000    7.80101  52.2%     -    0s\n",
      "     0     0    7.80101    0   29   16.31180    7.80101  52.2%     -    0s\n",
      "H    0     0                      16.2628544    7.80101  52.0%     -    0s\n",
      "H    0     0                      15.9880975    7.80101  51.2%     -    0s\n",
      "H    0     0                      15.7081129    7.80101  50.3%     -    0s\n",
      "     0     2    7.80101    0   29   15.70811    7.80101  50.3%     -    0s\n",
      "H 1390  1093                      15.5444898    8.44228  45.7%  11.6    0s\n",
      "H 1390  1093                      15.5368906    8.44228  45.7%  11.6    0s\n",
      "H 1390  1093                      15.5357286    8.44228  45.7%  11.6    0s\n",
      "H 1390  1091                      15.5339589    8.44228  45.7%  11.6    0s\n",
      "H 1393  1035                      15.5143201    8.44228  45.6%  11.6    0s\n",
      "H 1393   983                      15.5114375    8.44228  45.6%  11.6    0s\n",
      "H 1393   934                      15.5114213    8.44228  45.6%  11.6    0s\n",
      "H 1393   887                      15.5114095    8.44228  45.6%  11.6    0s\n",
      "H 1393   843                      15.5112715    8.44228  45.6%  11.6    0s\n",
      "H 1393   800                      15.5112351    8.44228  45.6%  11.6    0s\n",
      "H 1394   761                      15.4825660    8.44228  45.5%  11.6    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Flow cover: 7\n",
      "  RLT: 16\n",
      "  Relax-and-lift: 4\n",
      "  PSD: 235\n",
      "\n",
      "Explored 1403 nodes (17911 simplex iterations) in 1.16 seconds (1.03 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 15.4826 15.5112 15.5113 ... 15.5369\n",
      "\n",
      "Optimal solution found (tolerance 5.00e-02)\n",
      "Best objective 1.548256597506e+01, best bound 1.470906615643e+01, gap 4.9959%\n",
      "\n",
      "User-callback calls 3709, time in user-callback 0.02 sec\n"
     ]
    }
   ],
   "source": [
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "# Solution storage\n",
    "grb_model._feasible_solutions = []\n",
    "grb_model._relaxation_solutions = []\n",
    "grb_model.setParam(\"MIPGap\", 0.05)\n",
    "#model.setParam(\"NodeLimit\", 100)  # Explore a limited number of nodes\n",
    "\n",
    "# Optimize\n",
    "grb_model.optimize(my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.0, 0.0, 0.3328307282391785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1574644983911741, 0.25564451699144053, 0.0, 0.16105507040036132, 0.0, 0.0, 0.09300518597785103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 1.0, 0.0, -0.0, -0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, -0.0, 0.0, 1.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve optimal solution if available\n",
    "if grb_model.status == GRB.OPTIMAL:\n",
    "    optimal_solution = grb_model.getAttr('X', grb_model.getVars())\n",
    "    print(\"Optimal solution:\", optimal_solution)\n",
    "else:\n",
    "    print(f\"Model status: {grb_model.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n"
     ]
    }
   ],
   "source": [
    "## Extracting bounds\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "variable_bounds = {}\n",
    "for var in grb_model.getVars():\n",
    "    variable_bounds[var.VarName] = {'Lower': var.LB, 'Upper': var.UB}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Q, A, b, d, etc\n",
    "\n",
    "from parse_QP import parse_qplib_file\n",
    "\n",
    "# Replace '0031' with the desired file number\n",
    "data = parse_qplib_file('0031')\n",
    "\n",
    "# Access the data\n",
    "A = data['A']\n",
    "b_vector = data['b_vector']\n",
    "E = data['E']\n",
    "d_vector = data['d']\n",
    "Q = data['Q']\n",
    "variables_info = data['variables_info']\n",
    "variables_info = [v[0] for v in variables_info]\n",
    "binary_indices = data['binary_indices']\n",
    "variable_indices = data['variable_indices']\n",
    "\n",
    "m, n = A.shape\n",
    "# Get indices of non-zero elements in A\n",
    "row_indices, col_indices = np.nonzero(A)\n",
    "edge_weights = A[row_indices, col_indices]\n",
    "\n",
    "# Map variable types to numerical values\n",
    "# Node types: 0 - continuous, 1 - binary\n",
    "variable_types = np.array([0 if v[0] == 'x' else 1 for v in variables_info])\n",
    "\n",
    "# Collect indices of continuous and binary variables\n",
    "continuous_indices = np.where(variable_types == 0)[0]\n",
    "binary_indices = np.where(variable_types == 1)[0]\n",
    "n_continuous = len(continuous_indices)\n",
    "n_binary = len(binary_indices)\n",
    "n_variables = n_continuous + n_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing feasible solutions from file.\n",
      "Loaded existing infeasible solutions from file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from generate_solutions import generate_feasible_solutions, generate_infeasible_solutions\n",
    "\n",
    "# Set generate_new to False by default\n",
    "generate_new = False\n",
    "feasible_data_file = 'feasible_data.pkl'\n",
    "infeasible_data_file = 'infeasible_data.pkl'\n",
    "num_samples = 50000\n",
    "\n",
    "# Load or generate feasible solutions (as in your original code)\n",
    "if not generate_new and os.path.exists(feasible_data_file):\n",
    "    # Load feasible data\n",
    "    with open(feasible_data_file, 'rb') as f:\n",
    "        feasible_data = pickle.load(f)\n",
    "    feasible_solutions = feasible_data['solutions']\n",
    "    feasible_costs = feasible_data['costs']\n",
    "    print(\"Loaded existing feasible solutions from file.\")\n",
    "else:\n",
    "    # Generate feasible solutions\n",
    "    num_objectives = num_samples  # Adjust the number as needed\n",
    "    feasible_solutions, feasible_costs = generate_feasible_solutions(\n",
    "        A, E, Q, variables_info, b_vector, d_vector, num_objectives\n",
    "    )\n",
    "    # Save the generated data for future use\n",
    "    feasible_data = {'solutions': feasible_solutions, 'costs': feasible_costs}\n",
    "    with open(feasible_data_file, 'wb') as f:\n",
    "        pickle.dump(feasible_data, f)\n",
    "    print(\"Generated and saved feasible solutions.\")\n",
    "\n",
    "# Now, load or generate infeasible solutions\n",
    "if not generate_new and os.path.exists(infeasible_data_file):\n",
    "    # Load infeasible data\n",
    "    with open(infeasible_data_file, 'rb') as f:\n",
    "        infeasible_data = pickle.load(f)\n",
    "    infeasible_solutions = infeasible_data['solutions']\n",
    "    infeasible_costs = infeasible_data['costs']\n",
    "    print(\"Loaded existing infeasible solutions from file.\")\n",
    "else:\n",
    "    # Generate infeasible solutions\n",
    "    num_infeasible_samples = num_samples  # Adjust as needed\n",
    "    infeasible_solutions, infeasible_costs = generate_infeasible_solutions(\n",
    "        A, E, variables_info, b_vector, d_vector, Q, num_infeasible_samples, feasible_solutions\n",
    "    )\n",
    "    # Save the generated data for future use\n",
    "    infeasible_data = {'solutions': infeasible_solutions, 'costs': infeasible_costs}\n",
    "    with open(infeasible_data_file, 'wb') as f:\n",
    "        pickle.dump(infeasible_data, f)\n",
    "    print(\"Generated and saved infeasible solutions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Test Loss: 2.9913, Recon Loss: 1.0035, Cost Loss: 0.9715, Constraint Loss: 1.0163\n",
      "----\n",
      "Epoch 1, Train Loss: 1.4661, Recon Loss: 0.1304, Cost Loss: 0.9970, Constraint Loss: 0.3388 \n",
      "Test Loss: 0.9930, Recon Loss: 0.0001, Cost Loss: 0.9298, Constraint Loss: 0.0630\n",
      "----\n",
      "----\n",
      "Epoch 2, Train Loss: 0.7753, Recon Loss: 0.0004, Cost Loss: 0.7355, Constraint Loss: 0.0394 \n",
      "Test Loss: 0.4280, Recon Loss: 0.0004, Cost Loss: 0.4097, Constraint Loss: 0.0179\n",
      "----\n",
      "----\n",
      "Epoch 3, Train Loss: 0.3087, Recon Loss: 0.0006, Cost Loss: 0.2972, Constraint Loss: 0.0110 \n",
      "Test Loss: 0.2158, Recon Loss: 0.0002, Cost Loss: 0.2014, Constraint Loss: 0.0142\n",
      "----\n",
      "----\n",
      "Epoch 4, Train Loss: 0.1682, Recon Loss: 0.0002, Cost Loss: 0.1579, Constraint Loss: 0.0101 \n",
      "Test Loss: 0.1173, Recon Loss: 0.0001, Cost Loss: 0.1026, Constraint Loss: 0.0146\n",
      "----\n",
      "----\n",
      "Epoch 5, Train Loss: 0.0958, Recon Loss: 0.0001, Cost Loss: 0.0854, Constraint Loss: 0.0103 \n",
      "Test Loss: 0.0709, Recon Loss: 0.0001, Cost Loss: 0.0566, Constraint Loss: 0.0141\n",
      "----\n",
      "----\n",
      "Epoch 6, Train Loss: 0.0629, Recon Loss: 0.0001, Cost Loss: 0.0527, Constraint Loss: 0.0100 \n",
      "Test Loss: 0.0492, Recon Loss: 0.0001, Cost Loss: 0.0359, Constraint Loss: 0.0132\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import numpy as np\n",
    "\n",
    "# Adjusted GNNModelObj to output variable node embeddings\n",
    "class GNNModelObj(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNNModelObj, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))\n",
    "        return x  # Return embeddings for all variable nodes\n",
    "\n",
    "# Adjusted GNNModelConstraints to output variable and constraint node embeddings\n",
    "class GNNModelConstraints(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNNModelConstraints, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight=edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight=edge_weight))\n",
    "        x_var = x[data.variable_mask]\n",
    "        x_constraints = x[~data.variable_mask]\n",
    "        return x_var, x_constraints  # Return embeddings for both variables and constraints\n",
    "\n",
    "# Define the JointGNN model\n",
    "class JointGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels_obj, hidden_channels_cons, decoder_hidden_channels):\n",
    "        super(JointGNN, self).__init__()\n",
    "        # Encoders\n",
    "        self.encoder_obj = GNNModelObj(hidden_channels_obj)\n",
    "        self.encoder_cons = GNNModelConstraints(hidden_channels_cons)\n",
    "        # Decoder for x reconstruction\n",
    "        concat_dim = hidden_channels_obj + hidden_channels_cons\n",
    "        self.decoder_x = torch.nn.Sequential(\n",
    "            torch.nn.Linear(concat_dim, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "        # Decoder for cost prediction\n",
    "        self.decoder_cost = torch.nn.Sequential(\n",
    "            torch.nn.Linear(concat_dim, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "        # Decoder for constraint violation prediction\n",
    "        self.decoder_constraints = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels_cons, decoder_hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(decoder_hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_obj, data_feas):\n",
    "        # Process data through encoders\n",
    "        x_obj = self.encoder_obj(data_obj)  # Embeddings for variables\n",
    "        x_cons_var, x_cons_constraints = self.encoder_cons(data_feas)  # Embeddings for variables and constraints\n",
    "        # Extract variable embeddings\n",
    "        x_obj_var = x_obj[data_obj.variable_mask]\n",
    "        # Concatenate per-variable embeddings\n",
    "        x_var = torch.cat([x_obj_var, x_cons_var], dim=1)\n",
    "        # Decode to reconstruct x\n",
    "        x_hat = self.decoder_x(x_var).squeeze()\n",
    "        # Compute mean pooling over variable nodes for cost prediction\n",
    "        batch = data_obj.batch[data_obj.variable_mask]\n",
    "        x_var_pooled = global_mean_pool(x_var, batch)  # [batch_size, concat_dim]\n",
    "        # Predict cost\n",
    "        predicted_cost = self.decoder_cost(x_var_pooled).squeeze()\n",
    "        # Decode to predict constraint violations\n",
    "        predicted_constraints = self.decoder_constraints(x_cons_constraints).squeeze()\n",
    "        return x_hat, predicted_cost, predicted_constraints\n",
    "\n",
    "# Custom dataset to return pairs of data_obj and data_feas\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, data_list_obj, data_list_feas):\n",
    "        assert len(data_list_obj) == len(data_list_feas)\n",
    "        self.data_list_obj = data_list_obj\n",
    "        self.data_list_feas = data_list_feas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_obj)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list_obj[idx], self.data_list_feas[idx]\n",
    "\n",
    "# Custom collate function for batching\n",
    "def joint_collate_fn(batch):\n",
    "    data_obj_list, data_feas_list = zip(*batch)\n",
    "    batch_obj = Batch.from_data_list(data_obj_list)\n",
    "    batch_feas = Batch.from_data_list(data_feas_list)\n",
    "    return batch_obj, batch_feas\n",
    "\n",
    "# Data preparation functions\n",
    "def prepare_edge_index_and_attr(Q):\n",
    "    n_variables = Q.shape[0]\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    Q = Q if isinstance(Q, np.ndarray) else Q.toarray()\n",
    "    for i in range(n_variables):\n",
    "        for j in range(n_variables):\n",
    "            if Q[i, j] != 0:\n",
    "                edge_index.append([i, j])\n",
    "                edge_attr.append(Q[i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def prepare_constraint_edge_data(A, E, n_variables):\n",
    "    # A: m x n, E: p x n\n",
    "    m = A.shape[0]\n",
    "    p = E.shape[0]\n",
    "    num_constraints = m + p\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    # Edges from variables to inequality constraints\n",
    "    for constraint_idx in range(m):\n",
    "        for variable_idx in range(n_variables):\n",
    "            coeff = A[constraint_idx, variable_idx]\n",
    "            if coeff != 0:\n",
    "                # Edge from variable to constraint\n",
    "                edge_index.append([variable_idx, n_variables + constraint_idx])\n",
    "                edge_attr.append(coeff)\n",
    "    # Edges from variables to equality constraints\n",
    "    for constraint_idx in range(p):\n",
    "        for variable_idx in range(n_variables):\n",
    "            coeff = E[constraint_idx, variable_idx]\n",
    "            if coeff != 0:\n",
    "                # Edge from variable to constraint\n",
    "                edge_index.append([variable_idx, n_variables + m + constraint_idx])\n",
    "                edge_attr.append(coeff)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def create_data_list_obj(feasible_solutions, feasible_costs, edge_index, edge_attr):\n",
    "    data_list = []\n",
    "    for x_sol, cost_sol in zip(feasible_solutions, feasible_costs):\n",
    "        x = torch.tensor(x_sol, dtype=torch.float).unsqueeze(-1)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.variable_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        data.y_x = x.squeeze()\n",
    "        data.y_cost = torch.tensor([cost_sol], dtype=torch.float)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def create_data_list_feas(feasible_solutions, feasible_costs, A, E, b_vector, d_vector, edge_index, edge_attr, n_variables):\n",
    "    data_list = []\n",
    "    m = A.shape[0]\n",
    "    p = E.shape[0]\n",
    "    num_constraints = m + p\n",
    "    for x_sol, cost_sol in zip(feasible_solutions, feasible_costs):\n",
    "        x_vars = torch.tensor(x_sol, dtype=torch.float).unsqueeze(-1)\n",
    "        # Constraint node features: b_vector and d_vector\n",
    "        b = torch.tensor(b_vector, dtype=torch.float).unsqueeze(-1)\n",
    "        d = torch.tensor(d_vector, dtype=torch.float).unsqueeze(-1)\n",
    "        x_constraints = torch.cat([b, d], dim=0)\n",
    "        x_total = torch.cat([x_vars, x_constraints], dim=0)\n",
    "        data = Data(x=x_total, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.variable_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        data.variable_mask[:n_variables] = True  # Variables are first\n",
    "        data.y_x = x_vars.squeeze()\n",
    "        data.y_cost = torch.tensor([cost_sol], dtype=torch.float)\n",
    "\n",
    "        # Compute constraint violations\n",
    "        A_tensor = torch.tensor(A, dtype=torch.float)\n",
    "        E_tensor = torch.tensor(E, dtype=torch.float)\n",
    "        inequality_violations = torch.mv(A_tensor, x_vars.squeeze()) - torch.tensor(b_vector, dtype=torch.float)\n",
    "        equality_violations = torch.mv(E_tensor, x_vars.squeeze()) - torch.tensor(d_vector, dtype=torch.float)\n",
    "        y_constraints = torch.cat([inequality_violations, equality_violations], dim=0)\n",
    "        data.y_constraints = y_constraints  # [num_constraints]\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "def normalize_node_features(data_list):\n",
    "    # Concatenate all node features from all graphs\n",
    "    x_all = torch.cat([data.x for data in data_list], dim=0)\n",
    "    mean = x_all.mean(dim=0)\n",
    "    std = x_all.std(dim=0) + 1e-6  # Avoid division by zero\n",
    "    for data in data_list:\n",
    "        data.x = (data.x - mean) / std\n",
    "    return data_list, mean, std\n",
    "\n",
    "def normalize_targets(data_list):\n",
    "    # Normalize y_x (node targets)\n",
    "    y_x_all = torch.cat([data.y_x for data in data_list], dim=0)\n",
    "    mean_y_x = y_x_all.mean()\n",
    "    std_y_x = y_x_all.std() + 1e-6\n",
    "    for data in data_list:\n",
    "        data.y_x = (data.y_x - mean_y_x) / std_y_x\n",
    "\n",
    "    # Normalize y_cost (graph targets)\n",
    "    y_cost_all = torch.cat([data.y_cost for data in data_list], dim=0)\n",
    "    mean_y_cost = y_cost_all.mean()\n",
    "    std_y_cost = y_cost_all.std() + 1e-6\n",
    "    for data in data_list:\n",
    "        data.y_cost = (data.y_cost - mean_y_cost) / std_y_cost\n",
    "\n",
    "    # Initialize means and stds for y_constraints\n",
    "    mean_y_constraints = None\n",
    "    std_y_constraints = None\n",
    "\n",
    "    # Normalize y_constraints if present\n",
    "    if hasattr(data_list[0], 'y_constraints'):\n",
    "        y_constraints_all = torch.cat([data.y_constraints for data in data_list], dim=0)\n",
    "        mean_y_constraints = y_constraints_all.mean()\n",
    "        std_y_constraints = y_constraints_all.std() + 1e-6\n",
    "        for data in data_list:\n",
    "            data.y_constraints = (data.y_constraints - mean_y_constraints) / std_y_constraints\n",
    "\n",
    "    return data_list, (mean_y_x, std_y_x), (mean_y_cost, std_y_cost), (mean_y_constraints, std_y_constraints)\n",
    "\n",
    "def split_data(data_list, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(data_list))\n",
    "    test_set_size = int(len(data_list) * test_size)\n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "    train_data = [data_list[i] for i in train_indices]\n",
    "    test_data = [data_list[i] for i in test_indices]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Prepare edge information from Q\n",
    "edge_index_obj, edge_attr_obj = prepare_edge_index_and_attr(Q)\n",
    "\n",
    "# Prepare edge information from A and E\n",
    "edge_index_feas, edge_attr_feas = prepare_constraint_edge_data(A, E, n_variables)\n",
    "\n",
    "# Create Data objects\n",
    "data_list_obj = create_data_list_obj(feasible_solutions, feasible_costs, edge_index_obj, edge_attr_obj)\n",
    "data_list_feas = create_data_list_feas(feasible_solutions, feasible_costs, A, E, b_vector, d_vector, edge_index_feas, edge_attr_feas, n_variables)\n",
    "\n",
    "# Normalize node features for data_list_obj\n",
    "data_list_obj, mean_obj, std_obj = normalize_node_features(data_list_obj)\n",
    "\n",
    "# Normalize node features for data_list_feas\n",
    "data_list_feas, mean_feas, std_feas = normalize_node_features(data_list_feas)\n",
    "\n",
    "# Normalize targets (x and cost) for data_list_obj\n",
    "data_list_obj, (mean_y_x, std_y_x), (mean_y_cost, std_y_cost), _ = normalize_targets(data_list_obj)\n",
    "\n",
    "# Normalize targets (x, cost, and constraints) for data_list_feas\n",
    "data_list_feas, _, _, (mean_y_constraints, std_y_constraints) = normalize_targets(data_list_feas)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data_obj, test_data_obj = split_data(data_list_obj, test_size=0.2, random_state=42)\n",
    "train_data_feas, test_data_feas = split_data(data_list_feas, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = JointDataset(train_data_obj, train_data_feas)\n",
    "test_dataset = JointDataset(test_data_obj, test_data_feas)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=joint_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=joint_collate_fn)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = JointGNN(hidden_channels_obj=256, hidden_channels_cons=128, decoder_hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# Training and testing functions\n",
    "# Update the training function\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    total_constraint_loss = 0\n",
    "    total_original_cost_loss = 0\n",
    "    total_original_constraint_loss = 0\n",
    "    for data_obj_batch, data_feas_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "        # Get target x values and cost values\n",
    "        y_x = data_obj_batch.y_x\n",
    "        y_cost = data_obj_batch.y_cost.squeeze()\n",
    "        y_constraints = data_feas_batch.y_constraints\n",
    "        # Compute reconstruction loss\n",
    "        recon_loss = criterion(x_hat, y_x)\n",
    "        # Compute cost prediction loss\n",
    "        cost_loss = criterion(predicted_cost, y_cost)\n",
    "        # Compute constraint violation loss\n",
    "        constraint_loss = criterion(predicted_constraints, y_constraints)\n",
    "        # Total loss (weighted sum)\n",
    "        loss = recon_loss + cost_loss + constraint_loss  # You can adjust weights if desired\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data_obj_batch.num_graphs\n",
    "        total_recon_loss += recon_loss.item() * data_obj_batch.num_graphs\n",
    "        total_cost_loss += cost_loss.item() * data_obj_batch.num_graphs\n",
    "        total_constraint_loss += constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "        # Compute original cost loss (denormalized)\n",
    "        predicted_cost_orig = predicted_cost * std_y_cost + mean_y_cost\n",
    "        y_cost_orig = y_cost * std_y_cost + mean_y_cost\n",
    "        original_cost_loss = criterion(predicted_cost_orig, y_cost_orig)\n",
    "        total_original_cost_loss += original_cost_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "        # Compute original constraint loss (denormalized)\n",
    "        predicted_constraints_orig = predicted_constraints * std_y_constraints + mean_y_constraints\n",
    "        y_constraints_orig = y_constraints * std_y_constraints + mean_y_constraints\n",
    "        original_constraint_loss = criterion(predicted_constraints_orig, y_constraints_orig)\n",
    "        total_original_constraint_loss += original_constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_cost_loss = total_cost_loss / len(train_loader.dataset)\n",
    "    avg_constraint_loss = total_constraint_loss / len(train_loader.dataset)\n",
    "    avg_original_cost_loss = total_original_cost_loss / len(train_loader.dataset)\n",
    "    avg_original_constraint_loss = total_original_constraint_loss / len(train_loader.dataset)\n",
    "    return avg_loss, avg_recon_loss, avg_cost_loss, avg_constraint_loss, avg_original_cost_loss, avg_original_constraint_loss\n",
    "\n",
    "# Update the testing function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cost_loss = 0\n",
    "    total_constraint_loss = 0\n",
    "    total_original_cost_loss = 0\n",
    "    total_original_constraint_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data_obj_batch, data_feas_batch in loader:\n",
    "            x_hat, predicted_cost, predicted_constraints = model(data_obj_batch, data_feas_batch)\n",
    "            y_x = data_obj_batch.y_x\n",
    "            y_cost = data_obj_batch.y_cost.squeeze()\n",
    "            y_constraints = data_feas_batch.y_constraints\n",
    "            recon_loss = criterion(x_hat, y_x)\n",
    "            cost_loss = criterion(predicted_cost, y_cost)\n",
    "            constraint_loss = criterion(predicted_constraints, y_constraints)\n",
    "            loss = recon_loss + cost_loss + constraint_loss\n",
    "            total_loss += loss.item() * data_obj_batch.num_graphs\n",
    "            total_recon_loss += recon_loss.item() * data_obj_batch.num_graphs\n",
    "            total_cost_loss += cost_loss.item() * data_obj_batch.num_graphs\n",
    "            total_constraint_loss += constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "            # Compute original cost loss (denormalized)\n",
    "            predicted_cost_orig = predicted_cost * std_y_cost + mean_y_cost\n",
    "            y_cost_orig = y_cost * std_y_cost + mean_y_cost\n",
    "            original_cost_loss = criterion(predicted_cost_orig, y_cost_orig)\n",
    "            total_original_cost_loss += original_cost_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "            # Compute original constraint loss (denormalized)\n",
    "            predicted_constraints_orig = predicted_constraints * std_y_constraints + mean_y_constraints\n",
    "            y_constraints_orig = y_constraints * std_y_constraints + mean_y_constraints\n",
    "            original_constraint_loss = criterion(predicted_constraints_orig, y_constraints_orig)\n",
    "            total_original_constraint_loss += original_constraint_loss.item() * data_obj_batch.num_graphs\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(loader.dataset)\n",
    "    avg_cost_loss = total_cost_loss / len(loader.dataset)\n",
    "    avg_constraint_loss = total_constraint_loss / len(loader.dataset)\n",
    "    avg_original_cost_loss = total_original_cost_loss / len(loader.dataset)\n",
    "    avg_original_constraint_loss = total_original_constraint_loss / len(loader.dataset)\n",
    "    return avg_loss, avg_recon_loss, avg_cost_loss, avg_constraint_loss, avg_original_cost_loss, avg_original_constraint_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "test_loss, test_recon_loss, test_cost_loss, test_constraint_loss, test_orig_cost_loss, test_orig_constraint_loss = test(test_loader)\n",
    "\n",
    "print(f\"Pre-Test Loss: {test_loss:.4f}, Recon Loss: {test_recon_loss:.4f}, \"\n",
    "      f\"Cost Loss: {test_cost_loss:.4f}, Constraint Loss: {test_constraint_loss:.4f}\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Unpack all six returned values\n",
    "    train_loss, train_recon_loss, train_cost_loss, train_constraint_loss, train_orig_cost_loss, train_orig_constraint_loss = train()\n",
    "    test_loss, test_recon_loss, test_cost_loss, test_constraint_loss, test_orig_cost_loss, test_orig_constraint_loss = test(test_loader)\n",
    "\n",
    "    print(\"----\")\n",
    "    # Print out all the losses\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Recon Loss: {train_recon_loss:.4f}, \"\n",
    "          f\"Cost Loss: {train_cost_loss:.4f}, Constraint Loss: {train_constraint_loss:.4f} \")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Recon Loss: {test_recon_loss:.4f}, \"\n",
    "          f\"Cost Loss: {test_cost_loss:.4f}, Constraint Loss: {test_constraint_loss:.4f}\")\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5942)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(optimized_x[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
