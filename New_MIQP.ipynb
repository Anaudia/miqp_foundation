{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader  # Corrected import\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_mean_pool  # For pooling in the decoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt  # For plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_QP import my_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n",
      "Set parameter MIPGap to value 0.05\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (linux64 - \"Ubuntu 22.04.5 LTS\")\n",
      "\n",
      "CPU model: AMD EPYC 7513 32-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 64 physical cores, 128 logical processors, using up to 32 threads\n",
      "\n",
      "Non-default parameters:\n",
      "MIPGap  0.05\n",
      "\n",
      "Optimize a model with 32 rows, 60 columns and 120 nonzeros\n",
      "Model fingerprint: 0x00d24133\n",
      "Model has 464 quadratic objective terms\n",
      "Variable types: 30 continuous, 30 integer (30 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [3e+01, 2e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 5e+00]\n",
      "Found heuristic solution: objective 3654.4800000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 901 rows, 526 columns, 2321 nonzeros\n",
      "Presolved model has 30 quadratic constraint(s)\n",
      "Presolved model has 434 bilinear constraint(s)\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 496 continuous, 30 integer (30 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 1 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                      31.9607000    0.00000   100%     -    0s\n",
      "*    0     0               0      31.9607000    0.00000   100%     -    0s\n",
      "H    0     0                      20.8607250    0.00000   100%     -    0s\n",
      "*    0     0               0      20.8607250    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    2   20.86072    0.00000   100%     -    0s\n",
      "H    0     0                      18.2729563    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   18.27296    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   18.27296    0.00000   100%     -    0s\n",
      "H    0     0                      17.6415556    0.00000   100%     -    0s\n",
      "H    0     0                      17.4674945    0.00000   100%     -    0s\n",
      "H    0     0                      17.4402731    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   10   17.44027    0.00000   100%     -    0s\n",
      "     0     0    0.08623    0  149   17.44027    0.08623   100%     -    0s\n",
      "     0     0    1.81173    0   10   17.44027    1.81173  89.6%     -    0s\n",
      "     0     0    1.81174    0   17   17.44027    1.81174  89.6%     -    0s\n",
      "     0     0    2.62598    0   33   17.44027    2.62598  84.9%     -    0s\n",
      "     0     0    3.56723    0   25   17.44027    3.56723  79.5%     -    0s\n",
      "     0     0    3.93799    0   10   17.44027    3.93799  77.4%     -    0s\n",
      "     0     0    4.38555    0   22   17.44027    4.38555  74.9%     -    0s\n",
      "     0     0    5.55565    0   22   17.44027    5.55565  68.1%     -    0s\n",
      "     0     0    7.15470    0   27   17.44027    7.15470  59.0%     -    0s\n",
      "     0     0    7.69619    0   20   17.44027    7.69619  55.9%     -    0s\n",
      "     0     0    7.73872    0   29   17.44027    7.73872  55.6%     -    0s\n",
      "     0     0    7.78685    0   29   17.44027    7.78685  55.4%     -    0s\n",
      "     0     0    7.79280    0   29   17.44027    7.79280  55.3%     -    0s\n",
      "     0     0    7.80101    0   29   17.44027    7.80101  55.3%     -    0s\n",
      "H    0     0                      16.3118000    7.80101  52.2%     -    0s\n",
      "     0     0    7.80101    0   29   16.31180    7.80101  52.2%     -    0s\n",
      "H    0     0                      16.2628544    7.80101  52.0%     -    0s\n",
      "H    0     0                      15.9880975    7.80101  51.2%     -    0s\n",
      "H    0     0                      15.7081129    7.80101  50.3%     -    0s\n",
      "     0     2    7.80101    0   29   15.70811    7.80101  50.3%     -    0s\n",
      "H 2033  1602                      15.6653635    8.60559  45.1%  12.3    0s\n",
      "H 2190  1602                      15.6644219    8.71024  44.4%  12.4    0s\n",
      "H 2280  1533                      15.5343914    8.74770  43.7%  12.4    0s\n",
      "H 2284  1442                      15.4851673    8.74770  43.5%  12.4    0s\n",
      "H 2286  1370                      15.4734961   11.00119  28.9%  12.4    0s\n",
      "H 2287  1303                      15.4657935   12.37306  20.0%  12.4    1s\n",
      "H 2301  1246                      15.4645115   14.66410  5.18%  12.3    2s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 1\n",
      "  Flow cover: 4\n",
      "  RLT: 18\n",
      "  BQP: 1\n",
      "  PSD: 145\n",
      "\n",
      "Explored 2303 nodes (30934 simplex iterations) in 3.09 seconds (2.31 work units)\n",
      "Thread count was 32 (of 128 available processors)\n",
      "\n",
      "Solution count 10: 15.4645 15.4658 15.4735 ... 16.2629\n",
      "\n",
      "Optimal solution found (tolerance 5.00e-02)\n",
      "Best objective 1.546451154715e+01, best bound 1.469170048548e+01, gap 4.9973%\n",
      "\n",
      "User-callback calls 6138, time in user-callback 0.06 sec\n"
     ]
    }
   ],
   "source": [
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "# Solution storage\n",
    "grb_model._feasible_solutions = []\n",
    "grb_model._relaxation_solutions = []\n",
    "grb_model.setParam(\"MIPGap\", 0.05)\n",
    "#model.setParam(\"NodeLimit\", 100)  # Explore a limited number of nodes\n",
    "\n",
    "# Optimize\n",
    "grb_model.optimize(my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.0, 0.0, 0.49055482068657474, 0.0, 0.0, 0.09207045008492966, 0.0, 0.0, 0.06558348815658188, 0.2063472207852159, 0.0, 0.14544402028669787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 1.0, -0.0, -0.0, 1.0, 0.0, 0.0, 1.0, 1.0, -0.0, 1.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve optimal solution if available\n",
    "if grb_model.status == GRB.OPTIMAL:\n",
    "    optimal_solution = grb_model.getAttr('X', grb_model.getVars())\n",
    "    print(\"Optimal solution:\", optimal_solution)\n",
    "else:\n",
    "    print(f\"Model status: {grb_model.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file QPLIB_0031.lp\n",
      "Reading time = 0.00 seconds\n",
      "obj: 32 rows, 60 columns, 120 nonzeros\n"
     ]
    }
   ],
   "source": [
    "## Extracting bounds\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "# Read the problem\n",
    "number = \"0031\"\n",
    "grb_model = gp.read(f\"QPLIB_{number}.lp\")\n",
    "\n",
    "variable_bounds = {}\n",
    "for var in grb_model.getVars():\n",
    "    variable_bounds[var.VarName] = {'Lower': var.LB, 'Upper': var.UB}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Q, A, b, d, etc\n",
    "\n",
    "from parse_QP import parse_qplib_file\n",
    "\n",
    "# Replace '0031' with the desired file number\n",
    "data = parse_qplib_file('0031')\n",
    "\n",
    "# Access the data\n",
    "A = data['A']\n",
    "b_vector = data['b_vector']\n",
    "E = data['E']\n",
    "d = data['d']\n",
    "Q = data['Q']\n",
    "variables_info = data['variables_info']\n",
    "variables_info = [v[0] for v in variables_info]\n",
    "binary_indices = data['binary_indices']\n",
    "variable_indices = data['variable_indices']\n",
    "\n",
    "m, n = A.shape\n",
    "# Get indices of non-zero elements in A\n",
    "row_indices, col_indices = np.nonzero(A)\n",
    "edge_weights = A[row_indices, col_indices]\n",
    "\n",
    "# Map variable types to numerical values\n",
    "# Node types: 0 - continuous, 1 - binary\n",
    "variable_types = np.array([0 if v[0] == 'x' else 1 for v in variables_info])\n",
    "\n",
    "# Collect indices of continuous and binary variables\n",
    "continuous_indices = np.where(variable_types == 0)[0]\n",
    "binary_indices = np.where(variable_types == 1)[0]\n",
    "n_continuous = len(continuous_indices)\n",
    "n_binary = len(binary_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing feasible and infeasible solutions from files.\n"
     ]
    }
   ],
   "source": [
    "### Generating data and just reloading it if it's already generated (to save time)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from generate_solutions import generate_feasible_solutions, generate_infeasible_solutions\n",
    "\n",
    "# Set generate_new to False by default\n",
    "generate_new = False\n",
    "feasible_data_file = 'feasible_data.pkl'\n",
    "infeasible_data_file = 'infeasible_data.pkl'\n",
    "\n",
    "if not generate_new and os.path.exists(feasible_data_file) and os.path.exists(infeasible_data_file):\n",
    "    # Load feasible data\n",
    "    with open(feasible_data_file, 'rb') as f:\n",
    "        feasible_data = pickle.load(f)\n",
    "    feasible_solutions = feasible_data['solutions']\n",
    "    feasible_costs = feasible_data['costs']\n",
    "\n",
    "    # Load infeasible data\n",
    "    with open(infeasible_data_file, 'rb') as f:\n",
    "        infeasible_data = pickle.load(f)\n",
    "    infeasible_solutions = infeasible_data['solutions']\n",
    "    infeasible_costs = infeasible_data['costs']\n",
    "\n",
    "    print(\"Loaded existing feasible and infeasible solutions from files.\")\n",
    "else:\n",
    "    # Generate feasible solutions\n",
    "    num_objectives = 20000  # Adjust the number as needed\n",
    "    feasible_solutions, feasible_costs = generate_feasible_solutions(A, E, Q, variables_info, b_vector, d, num_objectives)\n",
    "\n",
    "    # Generate infeasible solutions\n",
    "    num_infeasible_samples = len(feasible_solutions) * 1  # For balance\n",
    "    infeasible_solutions, infeasible_costs = generate_infeasible_solutions(\n",
    "        A=A,\n",
    "        E=E,\n",
    "        variables_info=variables_info,\n",
    "        b_vector=b_vector,\n",
    "        d=d,\n",
    "        Q=Q,\n",
    "        num_infeasible_samples=num_infeasible_samples,\n",
    "        feasible_solutions=feasible_solutions\n",
    "    )\n",
    "\n",
    "    print(f\"Total unique solutions collected for both feasible/infeasible: {len(feasible_solutions) + len(infeasible_solutions)}\")\n",
    "\n",
    "    # Save feasible data\n",
    "    with open(feasible_data_file, 'wb') as f:\n",
    "        pickle.dump({'solutions': feasible_solutions, 'costs': feasible_costs}, f)\n",
    "\n",
    "    # Save infeasible data\n",
    "    with open(infeasible_data_file, 'wb') as f:\n",
    "        pickle.dump({'solutions': infeasible_solutions, 'costs': infeasible_costs}, f)\n",
    "\n",
    "    print(\"Generated new feasible and infeasible solutions and saved to files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feasible samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9297/9297 [00:02<00:00, 3272.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing infeasible samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9297/9297 [00:02<00:00, 3230.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 14875\n",
      "Validation samples: 3719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Creating the dataset (graph representation)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# Number of edge types\n",
    "num_edge_types = 3  # 0: ineq, 1: eq, 2: var-var\n",
    "\n",
    "def prepare_samples(x_samples, costs):\n",
    "    \"\"\"\n",
    "    Prepares samples by computing constraint violations and creating Data objects.\n",
    "\n",
    "    Args:\n",
    "        x_samples (list of np.ndarray): List of variable assignments.\n",
    "        costs (list of float): List of corresponding cost values.\n",
    "\n",
    "    Returns:\n",
    "        list of Data objects: Prepared data samples.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for x_combined, cost in tqdm(zip(x_samples, costs), total=len(x_samples)):\n",
    "        # Prepare node features and types\n",
    "        variable_node_features = x_combined.reshape(-1, 1)\n",
    "        variable_node_type = variable_types  # 0: continuous, 1: binary\n",
    "\n",
    "        # Prepare constraint node features and types\n",
    "        inequality_node_features = b_vector.reshape(-1, 1)\n",
    "        inequality_node_type = np.full(m, 2)  # Inequality constraint node type\n",
    "\n",
    "        equality_node_features = d.reshape(-1, 1)\n",
    "        equality_node_type = np.full(len(d), 3)  # Equality constraint node type\n",
    "\n",
    "        # Concatenate all node features and types\n",
    "        node_features = np.concatenate([variable_node_features,\n",
    "                                        inequality_node_features,\n",
    "                                        equality_node_features], axis=0)\n",
    "\n",
    "        node_types = np.concatenate([variable_node_type,\n",
    "                                     inequality_node_type,\n",
    "                                     equality_node_type], axis=0)\n",
    "\n",
    "        # Edge indices and attributes for inequality constraints\n",
    "        ineq_row_indices, ineq_col_indices = np.nonzero(A)\n",
    "        ineq_edge_weights = A[ineq_row_indices, ineq_col_indices]\n",
    "        ineq_source_nodes = ineq_col_indices\n",
    "        ineq_target_nodes = ineq_row_indices + n  # Shift indices for inequality nodes\n",
    "        ineq_edge_index = np.stack([ineq_source_nodes, ineq_target_nodes], axis=0)\n",
    "        ineq_edge_attr = ineq_edge_weights.reshape(-1, 1)\n",
    "\n",
    "        # Edge indices and attributes for equality constraints\n",
    "        eq_row_indices, eq_col_indices = np.nonzero(E)\n",
    "        eq_edge_weights = E[eq_row_indices, eq_col_indices]\n",
    "        eq_source_nodes = eq_col_indices\n",
    "        eq_target_nodes = eq_row_indices + n + m  # Shift indices for equality nodes\n",
    "        eq_edge_index = np.stack([eq_source_nodes, eq_target_nodes], axis=0)\n",
    "        eq_edge_attr = eq_edge_weights.reshape(-1, 1)\n",
    "\n",
    "        # Edge indices and attributes for variable-variable edges (from Q)\n",
    "        q_row_indices, q_col_indices = np.nonzero(Q)\n",
    "        q_edge_weights = Q[q_row_indices, q_col_indices]\n",
    "\n",
    "        # Include both directions since edges are directed\n",
    "        var_var_source_nodes = np.concatenate([q_row_indices, q_col_indices])\n",
    "        var_var_target_nodes = np.concatenate([q_col_indices, q_row_indices])\n",
    "        var_var_edge_index = np.stack([var_var_source_nodes,\n",
    "                                       var_var_target_nodes], axis=0)\n",
    "        var_var_edge_attr = np.concatenate([q_edge_weights, q_edge_weights]).reshape(-1, 1)\n",
    "\n",
    "        # Edge types\n",
    "        ineq_edge_type = np.full(len(ineq_edge_attr), 0)\n",
    "        eq_edge_type = np.full(len(eq_edge_attr), 1)\n",
    "        var_var_edge_type = np.full(len(var_var_edge_attr), 2)\n",
    "\n",
    "        # Combine edge indices, attributes, and types\n",
    "        edge_index = np.concatenate([ineq_edge_index,\n",
    "                                     eq_edge_index,\n",
    "                                     var_var_edge_index], axis=1)\n",
    "        edge_attr = np.concatenate([ineq_edge_attr,\n",
    "                                    eq_edge_attr,\n",
    "                                    var_var_edge_attr], axis=0)\n",
    "        edge_type = np.concatenate([ineq_edge_type,\n",
    "                                    eq_edge_type,\n",
    "                                    var_var_edge_type], axis=0)\n",
    "\n",
    "        # One-hot encode edge types\n",
    "        edge_type_onehot = np.zeros((len(edge_type), num_edge_types))\n",
    "        edge_type_onehot[np.arange(len(edge_type)), edge_type] = 1\n",
    "\n",
    "        # Concatenate edge attributes and edge type one-hot encoding\n",
    "        edge_attr = np.concatenate([edge_attr, edge_type_onehot], axis=1)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        node_type_tensor = torch.tensor(node_types, dtype=torch.long)\n",
    "        edge_index_torch = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edge_attr_torch = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        cost_tensor = torch.tensor([cost], dtype=torch.float)\n",
    "\n",
    "        # Compute inequality constraint violations: A x - b\n",
    "        Ax_minus_b = A @ x_combined - b_vector  # Shape: (m,)\n",
    "        inequality_violations = torch.tensor(Ax_minus_b, dtype=torch.float)\n",
    "\n",
    "        # Compute equality constraint violations: E x - d\n",
    "        Ex_minus_d = E @ x_combined - d  # Shape: (p,)\n",
    "        equality_violations = torch.tensor(Ex_minus_d, dtype=torch.float)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index_torch,\n",
    "            edge_attr=edge_attr_torch,\n",
    "            inequality_violations=inequality_violations,\n",
    "            equality_violations=equality_violations,\n",
    "            node_type=node_type_tensor,\n",
    "            cost=cost_tensor\n",
    "        )\n",
    "        samples.append(data)\n",
    "    return samples\n",
    "    \n",
    "print(\"Preparing feasible samples...\")\n",
    "feasible_samples = prepare_samples(\n",
    "    feasible_solutions, feasible_costs)\n",
    "\n",
    "print(\"Preparing infeasible samples...\")\n",
    "infeasible_samples = prepare_samples(\n",
    "    infeasible_solutions, infeasible_costs)\n",
    "\n",
    "# Combine samples\n",
    "data_list = feasible_samples + infeasible_samples\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# Split Dataset into Training and Validation\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(data_list) * train_ratio)\n",
    "train_data_list = data_list[:train_size]\n",
    "val_data_list = data_list[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(train_data_list)}\")\n",
    "print(f\"Validation samples: {len(val_data_list)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 64  # Adjust based on your memory capacity\n",
    "train_loader = DataLoader(train_data_list, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data_list, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▎         | 7/233 [00:00<00:03, 66.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 1.4498, KL Loss: 8.0630, Ineq Violation Loss: 1.0317, Eq Violation Loss: 1.6623, Cost Loss: 7126.6025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  48%|████▊     | 111/233 [00:01<00:01, 70.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.3074, KL Loss: 59.0498, Ineq Violation Loss: 0.0338, Eq Violation Loss: 0.3193, Cost Loss: 5774.2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  91%|█████████▏| 213/233 [00:03<00:00, 70.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2729, KL Loss: 33.6380, Ineq Violation Loss: 0.0266, Eq Violation Loss: 0.3655, Cost Loss: 5576.7002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 233/233 [00:03<00:00, 69.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 592.807292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   3%|▎         | 8/233 [00:00<00:03, 72.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.2718, KL Loss: 21.2140, Ineq Violation Loss: 0.0273, Eq Violation Loss: 0.2991, Cost Loss: 4312.2524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  48%|████▊     | 112/233 [00:01<00:01, 70.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.3892, KL Loss: 197.0537, Ineq Violation Loss: 0.1478, Eq Violation Loss: 0.6523, Cost Loss: 5816.7773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  91%|█████████ | 211/233 [00:03<00:00, 68.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.3139, KL Loss: 34.5658, Ineq Violation Loss: 0.0762, Eq Violation Loss: 0.1064, Cost Loss: 3426.8711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 233/233 [00:03<00:00, 69.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training Loss: 417.026484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   6%|▌         | 14/233 [00:00<00:03, 66.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.3255, KL Loss: 53.8299, Ineq Violation Loss: 0.0793, Eq Violation Loss: 0.0999, Cost Loss: 2782.2124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  48%|████▊     | 113/233 [00:01<00:01, 66.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.3159, KL Loss: 25.1604, Ineq Violation Loss: 0.0861, Eq Violation Loss: 0.1114, Cost Loss: 3011.2739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  90%|████████▉ | 209/233 [00:03<00:00, 71.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.3158, KL Loss: 24.0947, Ineq Violation Loss: 0.0793, Eq Violation Loss: 0.1161, Cost Loss: 2316.4695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 233/233 [00:03<00:00, 69.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training Loss: 338.154519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   3%|▎         | 7/233 [00:00<00:03, 64.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.3035, KL Loss: 19.1633, Ineq Violation Loss: 0.0735, Eq Violation Loss: 0.1134, Cost Loss: 2259.7317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  47%|████▋     | 110/233 [00:01<00:01, 68.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.3437, KL Loss: 41.7177, Ineq Violation Loss: 0.1349, Eq Violation Loss: 0.1620, Cost Loss: 3305.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  91%|█████████ | 211/233 [00:03<00:00, 69.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.3172, KL Loss: 16.4484, Ineq Violation Loss: 0.0913, Eq Violation Loss: 0.0582, Cost Loss: 2234.1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 233/233 [00:03<00:00, 68.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training Loss: 416.229309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   6%|▌         | 14/233 [00:00<00:03, 65.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.3044, KL Loss: 17.4925, Ineq Violation Loss: 0.0763, Eq Violation Loss: 0.0662, Cost Loss: 2623.5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  49%|████▉     | 115/233 [00:01<00:01, 67.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.3136, KL Loss: 18.9571, Ineq Violation Loss: 0.0815, Eq Violation Loss: 0.1107, Cost Loss: 2677.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  90%|████████▉ | 209/233 [00:03<00:00, 67.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2984, KL Loss: 12.4784, Ineq Violation Loss: 0.0674, Eq Violation Loss: 0.0773, Cost Loss: 2538.8948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 233/233 [00:03<00:00, 67.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training Loss: 322.171631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   3%|▎         | 7/233 [00:00<00:03, 61.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.3284, KL Loss: 30.0439, Ineq Violation Loss: 0.0882, Eq Violation Loss: 0.2635, Cost Loss: 3906.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  48%|████▊     | 112/233 [00:01<00:01, 65.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.3054, KL Loss: 8.2781, Ineq Violation Loss: 0.0686, Eq Violation Loss: 0.0670, Cost Loss: 2997.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  90%|█████████ | 210/233 [00:03<00:00, 64.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2932, KL Loss: 1.1657, Ineq Violation Loss: 0.0809, Eq Violation Loss: 0.4011, Cost Loss: 5046.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 233/233 [00:03<00:00, 64.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training Loss: 6327.995634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   6%|▌         | 14/233 [00:00<00:03, 66.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.2771, KL Loss: 3.2621, Ineq Violation Loss: 0.0297, Eq Violation Loss: 0.3571, Cost Loss: 4796.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  48%|████▊     | 112/233 [00:01<00:01, 65.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.2748, KL Loss: 2.1249, Ineq Violation Loss: 0.0256, Eq Violation Loss: 0.4408, Cost Loss: 5562.5127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  90%|█████████ | 210/233 [00:03<00:00, 62.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2707, KL Loss: 1.9084, Ineq Violation Loss: 0.0349, Eq Violation Loss: 0.3497, Cost Loss: 5060.8076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 233/233 [00:03<00:00, 65.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training Loss: 489.395313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   3%|▎         | 7/233 [00:00<00:03, 62.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.2693, KL Loss: 2.1897, Ineq Violation Loss: 0.0250, Eq Violation Loss: 0.3408, Cost Loss: 5426.3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  46%|████▋     | 108/233 [00:01<00:01, 68.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.2707, KL Loss: 1.7667, Ineq Violation Loss: 0.0278, Eq Violation Loss: 0.2961, Cost Loss: 4426.4307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  91%|█████████▏| 213/233 [00:03<00:00, 61.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2726, KL Loss: 1.8110, Ineq Violation Loss: 0.0252, Eq Violation Loss: 0.3710, Cost Loss: 5401.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 233/233 [00:03<00:00, 65.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training Loss: 475.553442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   6%|▌         | 14/233 [00:00<00:03, 67.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.2735, KL Loss: 1.8600, Ineq Violation Loss: 0.0277, Eq Violation Loss: 0.3716, Cost Loss: 4404.2354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  46%|████▋     | 108/233 [00:01<00:01, 67.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.2721, KL Loss: 1.7600, Ineq Violation Loss: 0.0274, Eq Violation Loss: 0.3577, Cost Loss: 4303.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  90%|█████████ | 210/233 [00:03<00:00, 67.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2709, KL Loss: 1.6329, Ineq Violation Loss: 0.0257, Eq Violation Loss: 0.2847, Cost Loss: 4329.2847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 233/233 [00:03<00:00, 66.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training Loss: 476.077346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   6%|▌         | 13/233 [00:00<00:03, 65.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Recon Loss: 0.2728, KL Loss: 1.3578, Ineq Violation Loss: 0.0251, Eq Violation Loss: 0.3511, Cost Loss: 4677.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  48%|████▊     | 112/233 [00:01<00:01, 65.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Recon Loss: 0.2765, KL Loss: 1.5898, Ineq Violation Loss: 0.0317, Eq Violation Loss: 0.2690, Cost Loss: 4728.2324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  90%|█████████ | 210/233 [00:03<00:00, 61.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 201, Recon Loss: 0.2725, KL Loss: 1.2980, Ineq Violation Loss: 0.0279, Eq Violation Loss: 0.3290, Cost Loss: 4478.4795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 233/233 [00:03<00:00, 64.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training Loss: 476.440666\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "from networks import Decoder, InequalityViolationPredictor, EqualityViolationPredictor, CostPredictor\n",
    "\n",
    "class ConstraintGNN(MessagePassing):\n",
    "    def __init__(self, input_node_feature_size, edge_feature_size, hidden_size):\n",
    "        super(ConstraintGNN, self).__init__(aggr='mean')  # Mean aggregation\n",
    "        self.message_mlp = nn.Sequential(\n",
    "            nn.Linear(input_node_feature_size + edge_feature_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(input_node_feature_size + hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        m_input = torch.cat([x_j, edge_attr], dim=1)\n",
    "        m = self.message_mlp(m_input)\n",
    "        return m\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        u = torch.cat([x, aggr_out], dim=1)\n",
    "        u = self.update_mlp(u)\n",
    "        return u\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, node_feature_size, edge_feature_size,\n",
    "                 hidden_size, latent_size, num_layers):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(ConstraintGNN(node_feature_size,\n",
    "                                         edge_feature_size, hidden_size))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(ConstraintGNN(hidden_size,\n",
    "                                             edge_feature_size, hidden_size))\n",
    "        self.global_pool = global_mean_pool\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "        # Global pooling\n",
    "        x = self.global_pool(x, batch)\n",
    "        # Compute mean and log variance for latent variables\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, node_feature_size, edge_feature_size,\n",
    "                 hidden_size, latent_size, num_layers,\n",
    "                 continuous_size, binary_size,\n",
    "                 num_inequality_constraints, num_equality_constraints):\n",
    "        super(VAEModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(node_feature_size, edge_feature_size,\n",
    "                                  hidden_size, latent_size, num_layers)\n",
    "        self.decoder = Decoder(latent_size, hidden_size,\n",
    "                               continuous_size, binary_size)\n",
    "        self.inequality_violation_predictor = InequalityViolationPredictor(\n",
    "            latent_size, num_inequality_constraints)\n",
    "        self.equality_violation_predictor = EqualityViolationPredictor(\n",
    "            latent_size, num_equality_constraints)\n",
    "        self.cost_predictor = CostPredictor(latent_size)\n",
    "        self.continuous_size = continuous_size\n",
    "        self.binary_size = binary_size\n",
    "        self.n = continuous_size + binary_size\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, node_type, batch = (\n",
    "            data.x, data.edge_index, data.edge_attr,\n",
    "            data.node_type, data.batch\n",
    "        )\n",
    "        mu, logvar = self.encoder(x, edge_index, edge_attr, batch)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        inequality_violation_pred = self.inequality_violation_predictor(z)\n",
    "        equality_violation_pred = self.equality_violation_predictor(z)\n",
    "        c_pred = self.cost_predictor(z)\n",
    "        return x_recon, mu, logvar, inequality_violation_pred, equality_violation_pred, c_pred\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 9. Instantiate the Model\n",
    "# ==============================\n",
    "\n",
    "# Number of edge types (as defined in dataset creation)\n",
    "num_edge_types = 3\n",
    "node_feature_size = 1  # One feature per node\n",
    "edge_feature_size = 1 + num_edge_types  # Edge weight + edge type one-hot\n",
    "hidden_size = 64 \n",
    "latent_size = 32  \n",
    "num_layers = 3  \n",
    "\n",
    "# Sizes for continuous and binary variables\n",
    "continuous_size = n_continuous  # Number of continuous variables\n",
    "binary_size = n_binary          # Number of binary variables\n",
    "\n",
    "# Number of constraints\n",
    "num_inequality_constraints = A.shape[0]  # m\n",
    "num_equality_constraints = E.shape[0]    # p\n",
    "\n",
    "model = VAEModel(\n",
    "    node_feature_size, edge_feature_size, hidden_size, latent_size, num_layers,\n",
    "    continuous_size, binary_size, num_inequality_constraints, num_equality_constraints\n",
    ")\n",
    "\n",
    "# Optionally move the model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Precompute indices tensors for variable types\n",
    "model.continuous_indices = torch.tensor(continuous_indices, dtype=torch.long).to(device)\n",
    "model.binary_indices = torch.tensor(binary_indices, dtype=torch.long).to(device)\n",
    "\n",
    "# ==============================\n",
    "# 5. Define Optimizer and Loss Functions\n",
    "# ==============================\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Loss weights\n",
    "beta = 1.0      # Weight for KL divergence loss\n",
    "gamma = 1.0     # Weight for inequality violation loss\n",
    "gamma_eq = 1.0  # Weight for equality violation loss\n",
    "delta = 0.1   # Weight for cost prediction loss\n",
    "\n",
    "# Loss functions\n",
    "reconstruction_loss_fn_continuous = nn.MSELoss()\n",
    "reconstruction_loss_fn_binary = nn.BCELoss()\n",
    "violation_loss_fn = nn.MSELoss() \n",
    "cost_loss_fn = nn.L1Loss()\n",
    "\n",
    "# ==============================\n",
    "# 6. Training the VAE\n",
    "# ==============================\n",
    "\n",
    "num_epochs = 10  # Adjust the number of epochs as needed\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        x_recon, mu, logvar, inequality_violation_pred, equality_violation_pred, c_pred = model(batch)\n",
    "        x = batch.x\n",
    "        node_type = batch.node_type\n",
    "        batch_size = batch.num_graphs\n",
    "\n",
    "        # Reshape x and node_type\n",
    "        x = x.view(batch_size, -1, 1)\n",
    "        node_type = node_type.view(batch_size, -1)\n",
    "\n",
    "        # Extract variable nodes\n",
    "        x_variable = x[:, :model.n, :].squeeze(-1)\n",
    "        node_type_variable = node_type[:, :model.n]\n",
    "\n",
    "        # Split reconstructed variables\n",
    "        x_recon_continuous = x_recon[:, :model.continuous_size]\n",
    "        x_recon_binary = x_recon[:, model.continuous_size:]\n",
    "\n",
    "        # Extract ground truth variables\n",
    "        x_variable_continuous = x_variable[:, continuous_indices]\n",
    "        x_variable_binary = x_variable[:, binary_indices]\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        recon_loss_continuous = reconstruction_loss_fn_continuous(x_recon_continuous, x_variable_continuous)\n",
    "        recon_loss_binary = reconstruction_loss_fn_binary(x_recon_binary, x_variable_binary)\n",
    "        recon_loss = recon_loss_continuous + recon_loss_binary\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # **Reshape constraint violations to match predictions**\n",
    "        actual_inequality_violations = batch.inequality_violations.view(batch_size, -1).to(device)\n",
    "        actual_equality_violations = batch.equality_violations.view(batch_size, -1).to(device)\n",
    "\n",
    "        # Inequality Violation Loss\n",
    "        inequality_violation_loss = violation_loss_fn(inequality_violation_pred, actual_inequality_violations)\n",
    "\n",
    "        # Equality Violation Loss\n",
    "        equality_violation_loss = violation_loss_fn(equality_violation_pred, actual_equality_violations)\n",
    "\n",
    "        # Cost Prediction Loss\n",
    "        c = batch.cost.to(device)\n",
    "        cost_loss = cost_loss_fn(c_pred.view(-1), c)\n",
    "\n",
    "        # Total Loss\n",
    "        loss = (\n",
    "            recon_loss +\n",
    "            beta * kl_loss +\n",
    "            gamma * inequality_violation_loss +\n",
    "            gamma_eq * equality_violation_loss +\n",
    "            delta * cost_loss\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Diagnostic Logging\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx+1}, Recon Loss: {recon_loss.item():.4f}, KL Loss: {kl_loss.item():.4f}, \"\n",
    "                  f\"Ineq Violation Loss: {inequality_violation_loss.item():.4f}, \"\n",
    "                  f\"Eq Violation Loss: {equality_violation_loss.item():.4f}, \"\n",
    "                  f\"Cost Loss: {cost_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
